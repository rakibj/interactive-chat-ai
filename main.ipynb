{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b0a5605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_bootstrap.py\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(1)\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f817fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GROQ_API_KEY loaded: True\n",
      "GROQ_API_KEY prefix: gsk_d\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from collections import deque\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "from openai import OpenAI\n",
    "from typing import Iterator\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\"  # Only show errors\n",
    "from llama_cpp import Llama\n",
    "\n",
    "try:\n",
    "    from pocket_tts import TTSModel\n",
    "    POCKET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POCKET_AVAILABLE = False\n",
    "    print(\"âš ï¸ pocket-tts not installed. Install with: uv add pocket-tts\")\n",
    "\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "TRANSCRIPTION_MODE = \"vosk\"  # Options: \"vosk\" (fast partials) or \"whisper\"\n",
    "PROJECT_ROOT = r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\"\n",
    "TTS_MODE = \"pocket\"  # Options: \"pocket\" (neural) or \"powershell\" (system)\n",
    "POCKET_VOICE = \"alba\"  # Options: alba, marius, javert, jean, fantine, cosette, \n",
    "\n",
    "# =============================\n",
    "# GLOBAL INTERRUPTION CONTROL\n",
    "# =============================\n",
    "human_interrupt_event = threading.Event()   # Human started speaking\n",
    "ai_speaking_event = threading.Event()       # AI currently speaking\n",
    "processing_lock = threading.Lock()           # Guards LLM/Whisper sections\n",
    "human_speaking_now = threading.Event()\n",
    "ai_interrupt_latched = False\n",
    "spoken_sentences = []\n",
    "tts_playback_lock = threading.Lock()\n",
    "\n",
    "\n",
    "# Carry-over buffer when human interrupts mid-processing\n",
    "pending_user_text = deque()  # list[str]\n",
    "\n",
    "# =============================\n",
    "# EPHEMERAL CONVERSATION MEMORY\n",
    "# =============================\n",
    "MAX_MEMORY_TURNS = 24  # 3 user + 3 assistant turns (tune later)\n",
    "\n",
    "conversation_memory = deque(maxlen=MAX_MEMORY_TURNS)\n",
    "\n",
    "# =============================\n",
    "# LLM BACKEND CONFIG\n",
    "# =============================\n",
    "\n",
    "LLM_BACKEND = \"groq\"\n",
    "# options: \"local\", \"groq\", \"deepseek\", \"openai\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "GROQ_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
    "DEEPSEEK_API_KEY = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# OpenAI-compatible endpoints\n",
    "OPENAI_BASE_URL = \"https://api.openai.com/v1\"\n",
    "GROQ_BASE_URL = \"https://api.groq.com/openai/v1\"\n",
    "DEEPSEEK_BASE_URL = \"https://api.deepseek.com/v1\"\n",
    "\n",
    "# Model names (examples â€“ adjust freely)\n",
    "OPENAI_MODEL = \"gpt-4o-mini\"\n",
    "GROQ_MODELS = {\n",
    "    \"best\": \"llama-3.1-70b-versatile\",\n",
    "    \"fast\": \"llama-3.1-8b-instant\",\n",
    "    \"cheap\": \"mixtral-8x7b-32768\",\n",
    "}\n",
    "GROQ_MODEL = GROQ_MODELS[\"fast\"]\n",
    "DEEPSEEK_MODEL = \"deepseek-chat\"\n",
    "\n",
    "print(\"GROQ_API_KEY loaded:\", bool(GROQ_API_KEY))\n",
    "print(\"GROQ_API_KEY prefix:\", GROQ_API_KEY[:5] if GROQ_API_KEY else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25a1a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PC/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (for final transcription)...\n",
      "Loading Vosk...\n",
      "ASR mode: vosk\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# LOAD MODELS (ALWAYS LOAD WHISPER FOR FINAL TRANSCRIPTION)\n",
    "# =============================\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "\n",
    "print(\"Loading Whisper (for final transcription)...\")\n",
    "whisper = WhisperModel(\n",
    "    r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\whisper\\distil-small.en\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\",\n",
    "    local_files_only=True,  # Force local, no hub download\n",
    "    cpu_threads=8\n",
    ")\n",
    "\n",
    "# Load Vosk only if needed\n",
    "vosk_model = None\n",
    "vosk_rec = None\n",
    "if TRANSCRIPTION_MODE == \"vosk\":\n",
    "    from vosk import Model, KaldiRecognizer\n",
    "    print(\"Loading Vosk...\")\n",
    "    vosk_model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    vosk_rec = KaldiRecognizer(vosk_model, 16000)\n",
    "    vosk_rec.SetWords(True)\n",
    "\n",
    "print(f\"ASR mode: {TRANSCRIPTION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6b0f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking model path: D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\llm\\qwen2.5-3b-instruct-q5_k_m.gguf\n",
      "   Exists? True\n",
      "   Size: 2.27 GB\n"
     ]
    }
   ],
   "source": [
    "_cloud_clients = {}\n",
    "\n",
    "def get_cloud_client(backend: str) -> OpenAI:\n",
    "    if backend in _cloud_clients:\n",
    "        return _cloud_clients[backend]\n",
    "\n",
    "    if backend == \"openai\":\n",
    "        client = OpenAI(api_key=OPENAI_API_KEY, base_url=OPENAI_BASE_URL)\n",
    "    elif backend == \"groq\":\n",
    "        client = OpenAI(api_key=GROQ_API_KEY, base_url=GROQ_BASE_URL)\n",
    "    elif backend == \"deepseek\":\n",
    "        client = OpenAI(api_key=DEEPSEEK_API_KEY, base_url=DEEPSEEK_BASE_URL)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown cloud backend: {backend}\")\n",
    "\n",
    "    _cloud_clients[backend] = client\n",
    "    return client\n",
    "\n",
    "\n",
    "def stream_chat_completion(messages, max_tokens, temperature) -> Iterator[str]:\n",
    "    \"\"\"\n",
    "    Unified streaming generator.\n",
    "    Yields text tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    if LLM_BACKEND == \"local\":\n",
    "        llm = get_llm()\n",
    "        stream = llm.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "        for chunk in stream:\n",
    "            delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "            if \"content\" in delta:\n",
    "                yield delta[\"content\"]\n",
    "\n",
    "    else:\n",
    "        client = get_cloud_client(LLM_BACKEND)\n",
    "\n",
    "        if LLM_BACKEND == \"openai\":\n",
    "            model = OPENAI_MODEL\n",
    "        elif LLM_BACKEND == \"groq\":\n",
    "            model = GROQ_MODEL\n",
    "        elif LLM_BACKEND == \"deepseek\":\n",
    "            model = DEEPSEEK_MODEL\n",
    "\n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            max_tokens=max_tokens,\n",
    "            temperature=temperature,\n",
    "            stream=True,\n",
    "        )\n",
    "\n",
    "        for event in stream:\n",
    "            if not event.choices:\n",
    "                continue\n",
    "\n",
    "            delta = event.choices[0].delta\n",
    "            if not delta:\n",
    "                continue\n",
    "\n",
    "            if hasattr(delta, \"content\") and delta.content is not None:\n",
    "                yield delta.content\n",
    "\n",
    "\n",
    "\n",
    "_llama_model = None\n",
    "GGUF_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"llm\" ,\"qwen2.5-3b-instruct-q5_k_m.gguf\")  # Adjust path as needed\n",
    "\n",
    "def get_llm():\n",
    "    global _llama_model\n",
    "    if _llama_model is None:\n",
    "        print(\"â³ Loading Qwen2.5-3B (Q5_K_M GGUF) on CPU...\")\n",
    "        try:\n",
    "            _llama_model = Llama(\n",
    "                model_path=GGUF_MODEL_PATH,\n",
    "                n_ctx=2048,\n",
    "                n_threads=8,\n",
    "                n_threads_batch=8,\n",
    "                n_batch=512,\n",
    "                n_gqa=1,  # âš ï¸ CRITICAL FOR QWEN\n",
    "                verbose=False,  # Enable loading logs\n",
    "                use_mmap=True,\n",
    "                use_mlock=False,\n",
    "                rope_freq_base=1000000.0\n",
    "            )\n",
    "            print(\"âœ… Qwen2.5-3B loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ FAILED to load GGUF model: {e}\")\n",
    "            print(f\"   Check if file exists: {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "            raise  # Force crash to see error\n",
    "    return _llama_model\n",
    "\n",
    "# Add after defining GGUF_MODEL_PATH\n",
    "print(f\"ğŸ” Checking model path: {GGUF_MODEL_PATH}\")\n",
    "print(f\"   Exists? {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "print(f\"   Size: {os.path.getsize(GGUF_MODEL_PATH) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "089cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR worker started\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# AUDIO SETUP\n",
    "# =============================\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = []\n",
    "tts_stream = None\n",
    "VOSK_MIN_SAMPLES = 3200  # 0.2 sec @ 16kHz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_buffer.append(indata.copy())\n",
    "\n",
    "stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ASR WORKER (STREAMING PARTIALS)\n",
    "# =============================\n",
    "asr_audio = deque()      # For streaming partials (trimmed)\n",
    "turn_audio = deque()     # For final transcription (full turn)\n",
    "asr_lock = threading.Lock()\n",
    "turn_audio_lock = threading.Lock()\n",
    "current_partial_text = \"\"\n",
    "vosk_reset_requested = False\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def asr_worker():\n",
    "    global current_partial_text, vosk_reset_requested\n",
    "    WHISPER_WINDOW_SEC = 1.2\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05 if TRANSCRIPTION_MODE == \"vosk\" else 0.7)\n",
    "\n",
    "        if TRANSCRIPTION_MODE == \"whisper\":\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                now = time.time()\n",
    "                recent = [frame for frame, t in asr_audio if now - t <= WHISPER_WINDOW_SEC]\n",
    "            if not recent:\n",
    "                continue\n",
    "            audio_np = np.concatenate(recent)\n",
    "            segments, _ = whisper.transcribe(\n",
    "                audio_np, language=\"en\", vad_filter=False, beam_size=1, temperature=0.0\n",
    "            )\n",
    "            text = \" \".join(seg.text for seg in segments).strip()\n",
    "            if text and text != current_partial_text:\n",
    "                current_partial_text = text\n",
    "                print(\"ğŸ“ Partial:\", text)\n",
    "\n",
    "        else:  # Vosk mode\n",
    "            if vosk_reset_requested:\n",
    "                vosk_rec.Reset()\n",
    "                vosk_reset_requested = False\n",
    "                current_partial_text = \"\"\n",
    "                with vosk_partial_lock:\n",
    "                    vosk_partial_for_interruption = \"\"  # Clear interruption buffer too\n",
    "            \n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                frame, _ = asr_audio.popleft()\n",
    "                if len(frame) < VOSK_MIN_SAMPLES:\n",
    "                    continue\n",
    "                pcm16 = float32_to_int16(frame)\n",
    "            \n",
    "            try:\n",
    "                if vosk_rec.AcceptWaveform(pcm16.tobytes()):\n",
    "                    # Final result - update interruption buffer with reliable text\n",
    "                    res = json.loads(vosk_rec.Result())\n",
    "                    text = res.get(\"text\", \"\").strip()\n",
    "                    if text:\n",
    "                        print(f\"ğŸ“ Final: {text}\")\n",
    "                        current_partial_text = \"\"\n",
    "                        with vosk_partial_lock:\n",
    "                            vosk_partial_for_interruption = text  # More reliable than partials\n",
    "                else:\n",
    "                    # Partial result - update for low-latency interruption detection\n",
    "                    res = json.loads(vosk_rec.PartialResult())\n",
    "                    partial = res.get(\"partial\", \"\").strip()\n",
    "                    if partial and partial != current_partial_text:\n",
    "                        current_partial_text = partial\n",
    "                        print(f\"ğŸ“ Partial: {partial}\")\n",
    "                        with vosk_partial_lock:\n",
    "                            # CRITICAL: Only update if contains actual words (filter \"uh\"/\"ah\" artifacts)\n",
    "                            if len(partial.split()) >= 1:  # At least 1 real word\n",
    "                                vosk_partial_for_interruption = partial\n",
    "            \n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "threading.Thread(target=asr_worker, daemon=True).start()\n",
    "print(\"ASR worker started\")\n",
    "\n",
    "# =============================\n",
    "# TURN-TAKING RULES\n",
    "# =============================\n",
    "TRAILING_CONJUNCTIONS = {\"and\",\"or\",\"but\",\"because\",\"so\",\"that\",\"which\",\"who\",\"when\",\"if\",\"though\",\"while\"}\n",
    "OPEN_ENDED_PREFIXES = (\"i think\",\"i guess\",\"i'm not sure\",\"the thing is\",\"it depends\")\n",
    "QUESTION_LEADINS = (\"do you think\",\"would you say\",\"is it possible\",\"can you\")\n",
    "SELF_REPAIR_MARKERS = (\"i mean\",\"actually\",\"sorry\",\"no wait\")\n",
    "FILLER_ENDINGS = (\"uh\",\"um\",\"like\",\"you know\",\"kind of\")\n",
    "\n",
    "def lexical_bias(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    t = text.lower().strip()\n",
    "    words = t.split()\n",
    "    score = 0.0\n",
    "    if words[-1] in TRAILING_CONJUNCTIONS: score -= 1.0\n",
    "    if any(t.startswith(p) for p in OPEN_ENDED_PREFIXES): score -= 0.6\n",
    "    if any(t.startswith(q) for q in QUESTION_LEADINS): score -= 0.5\n",
    "    if any(m in t[-20:] for m in SELF_REPAIR_MARKERS): score -= 0.4\n",
    "    if words[-1] in FILLER_ENDINGS: score -= 0.7\n",
    "    return score\n",
    "\n",
    "def energy_decay_score(energy_history):\n",
    "    if len(energy_history) < 5: return 0.0\n",
    "    x = np.arange(len(energy_history))\n",
    "    y = np.array(energy_history)\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    return 0.8 if slope < -0.00015 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a017d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# POCKET TTS LOADING\n",
    "# =============================\n",
    "_pocket_model = None\n",
    "_pocket_voice_state = None\n",
    "_pocket_sample_rate = 24000  # Pocket TTS default\n",
    "# Interruption sensitivity: 0.0 = strict speech detection (1+ words), 1.0 = energy-only (current behavior)\n",
    "INTERRUPTION_SENSITIVITY = 0.0  # Tune between 0.0 (strict) and 1.0 (responsive)\n",
    "\n",
    "# Minimum words required for interruption at sensitivity=0.0\n",
    "MIN_WORDS_FOR_INTERRUPT = 1  # Require at least 1 actual word\n",
    "\n",
    "# Vosk partial text buffer for interruption detection (thread-safe)\n",
    "vosk_partial_for_interruption = \"\"\n",
    "vosk_partial_lock = threading.Lock()\n",
    "\n",
    "def reset_tts_state_on_interruption():\n",
    "    \"\"\"CRITICAL: Reset all TTS state when human interrupts\"\"\"\n",
    "    global spoken_sentences, sentence_buffer\n",
    "    \n",
    "    with tts_playback_lock:\n",
    "        # 1. Clear partial sentence buffers to prevent re-speaking fragments\n",
    "        spoken_sentences.clear()\n",
    "        sentence_buffer = \"\"\n",
    "        \n",
    "        # 2. Abort current playback (redundant safety with chunked playback)\n",
    "        if tts_stream is not None and tts_stream.active:\n",
    "            try:\n",
    "                tts_stream.abort()\n",
    "                tts_stream.close()\n",
    "            except:\n",
    "                pass\n",
    "            tts_stream = None\n",
    "        \n",
    "        # 3. Clear pending queue AGAIN (race condition safety)\n",
    "        with response_queue.mutex:\n",
    "            response_queue.queue.clear()\n",
    "        \n",
    "        print(\"ğŸ§¹ TTS state fully reset on interruption\")\n",
    "\n",
    "def get_pocket_tts():\n",
    "    \"\"\"Lazy load Pocket TTS model\"\"\"\n",
    "    global _pocket_model, _pocket_voice_state, _pocket_sample_rate\n",
    "    if _pocket_model is None:\n",
    "        if not POCKET_AVAILABLE:\n",
    "            raise ImportError(\"pocket-tts not installed\")\n",
    "        print(f\"â³ Loading Pocket TTS (voice: {POCKET_VOICE})...\")\n",
    "        _pocket_model = TTSModel.load_model()\n",
    "        _pocket_voice_state = _pocket_model.get_state_for_audio_prompt(POCKET_VOICE)\n",
    "        _pocket_sample_rate = _pocket_model.sample_rate\n",
    "        print(\"âœ… Pocket TTS loaded!\")\n",
    "    return _pocket_model, _pocket_voice_state, _pocket_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5f5c3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def speak(text):\n",
    "    global tts_stream, ai_interrupt_latched\n",
    "    if not text.strip():\n",
    "        return\n",
    "\n",
    "    with tts_playback_lock:\n",
    "        ai_speaking_event.set()\n",
    "        ai_interrupt_latched = False\n",
    "\n",
    "    try:\n",
    "        model, voice_state, sr = get_pocket_tts()\n",
    "        audio = model.generate_audio(voice_state, text)\n",
    "        audio_np = audio.numpy() if hasattr(audio, 'numpy') else np.array(audio)\n",
    "\n",
    "        # Play in 100ms chunks with interruption checks\n",
    "        chunk_size = int(sr * 0.1)  # 100ms chunks\n",
    "        tts_stream = sd.OutputStream(samplerate=sr, channels=1, dtype='float32')\n",
    "        tts_stream.start()\n",
    "        \n",
    "        for i in range(0, len(audio_np), chunk_size):\n",
    "            # CRITICAL: Check for interruption BEFORE each chunk\n",
    "            if human_speaking_now.is_set() or human_interrupt_event.is_set():\n",
    "                print(\"ğŸ›‘ TTS interrupted mid-playback\")\n",
    "                ai_interrupt_latched = True\n",
    "                reset_tts_state_on_interruption()\n",
    "                break\n",
    "                \n",
    "            chunk = audio_np[i:i+chunk_size]\n",
    "            tts_stream.write(chunk)\n",
    "            \n",
    "        tts_stream.stop()\n",
    "        tts_stream.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”Š TTS error: {e}\")\n",
    "    finally:\n",
    "        with tts_playback_lock:\n",
    "            ai_speaking_event.clear()\n",
    "            if tts_stream is not None:\n",
    "                try:\n",
    "                    tts_stream.close()\n",
    "                except:\n",
    "                    pass\n",
    "            tts_stream = None\n",
    "\n",
    "def speak_powershell(text):\n",
    "    \"\"\"Original Windows PowerShell TTS (fallback)\"\"\"\n",
    "    safe_text = text.replace('\"', '\"\"').replace('\\n', ' ').replace('\\r', '')\n",
    "    cmd = f'Add-Type -AssemblyName System.Speech; $s=New-Object System.Speech.Synthesis.SpeechSynthesizer; $s.Speak(\"{safe_text}\")'\n",
    "    try:\n",
    "        subprocess.run([\"powershell\", \"-Command\", cmd],\n",
    "                       stdout=subprocess.DEVNULL,\n",
    "                       stderr=subprocess.DEVNULL,\n",
    "                       timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”Š Speech error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d8cb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# WINDOWS-RELIABLE TTS (POWER SHELL)\n",
    "# =============================\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def tts_main_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            text = response_queue.get(timeout=0.1)\n",
    "\n",
    "            # If human spoke, drop all pending AI speech\n",
    "            if human_interrupt_event.is_set():\n",
    "                with response_queue.mutex:\n",
    "                    response_queue.queue.clear()\n",
    "                continue\n",
    "\n",
    "            speak(text)\n",
    "\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Started earlier in your code:\n",
    "threading.Thread(target=tts_main_loop, daemon=False).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e1d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the TOP of your notebook (before main loop), make sure these exist:\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class TurnTiming:\n",
    "    turn_id: int = 0\n",
    "    speech_end_time: float = 0.0\n",
    "    audio_capture_duration_ms: float = 0.0\n",
    "    whisper_transcribe_ms: float = 0.0\n",
    "    whisper_rtf: float = 0.0\n",
    "    llm_tokenize_ms: float = 0.0\n",
    "    llm_generate_ms: float = 0.0\n",
    "    llm_tokens_per_sec: float = 0.0\n",
    "    text_process_ms: float = 0.0\n",
    "    tts_generate_ms: float = 0.0\n",
    "    tts_playback_ms: float = 0.0\n",
    "    total_latency_ms: float = 0.0\n",
    "    total_audio_duration_sec: float = 0.0\n",
    "    \n",
    "    def print_report(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“Š TURN #{self.turn_id} TIMING AUDIT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ğŸ™ï¸  User audio duration:     {self.total_audio_duration_sec:.2f}s\")\n",
    "        print(f\"â±ï¸  Speech end â†’ Response:   {self.total_latency_ms:.0f}ms total\")\n",
    "        print(f\"{'â”€'*40}\")\n",
    "        print(f\"1. Audio buffer capture:     {self.audio_capture_duration_ms:.1f}ms\")\n",
    "        print(f\"2. Whisper transcription:    {self.whisper_transcribe_ms:.1f}ms (RTF: {self.whisper_rtf:.2f}x)\")\n",
    "        print(f\"3. LLM tokenization:         {self.llm_tokenize_ms:.1f}ms\")\n",
    "        print(f\"4. LLM generation:           {self.llm_generate_ms:.1f}ms ({self.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        print(f\"5. Text processing:          {self.text_process_ms:.1f}ms\")\n",
    "        if self.tts_generate_ms > 0:\n",
    "            print(f\"6. TTS generation:           {self.tts_generate_ms:.1f}ms\")\n",
    "            print(f\"7. Audio playback:           {self.tts_playback_ms:.1f}ms\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "turn_counter = 0\n",
    "timing_history: List[TurnTiming] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8931e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(frames, timing: TurnTiming):\n",
    "    global turn_counter\n",
    "\n",
    "    # âœ… CRITICAL: Abort if human is STILL speaking after interruption\n",
    "    if human_speaking_now.is_set():\n",
    "        print(\"âš ï¸ Skipping generation â€” human still speaking after interrupt\")\n",
    "        with turn_audio_lock:\n",
    "            turn_audio.clear()\n",
    "        pending_user_text.append(\" \".join([f[0].tobytes().decode('latin1')[:50] for f in frames])[:100] + \"...\")\n",
    "        return\n",
    "    \n",
    "    with processing_lock:\n",
    "        timing.speech_end_time = time.perf_counter()\n",
    "    \n",
    "    # Skip only if human is actively speaking NOW\n",
    "        if human_speaking_now.is_set():\n",
    "            print(\"âš ï¸ Skipping generation â€” human speaking\")\n",
    "            return\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Audio Capture\n",
    "        t0 = time.perf_counter()\n",
    "        if not frames:\n",
    "            print(\"âš ï¸ No audio captured â€” skipping response\")\n",
    "            return\n",
    "        \n",
    "        full_audio = np.concatenate([frame for frame, _ in frames])\n",
    "        timing.total_audio_duration_sec = full_audio.shape[0] / 16000.0\n",
    "        timing.audio_capture_duration_ms = (time.perf_counter() - t0) * 1000\n",
    "        print(f\"ğŸ”Š Captured {len(frames)} frames ({timing.total_audio_duration_sec:.2f}s) in {timing.audio_capture_duration_ms:.1f}ms\")\n",
    "        \n",
    "        # Stage 2: Whisper Transcription\n",
    "        t1 = time.perf_counter()\n",
    "        segments, info = whisper.transcribe(\n",
    "            full_audio,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            condition_on_previous_text=False\n",
    "        )\n",
    "        user_text = \" \".join(seg.text for seg in segments).strip()\n",
    "        if human_interrupt_event.is_set():\n",
    "            print(\"ğŸ§  Interrupted during transcription, buffering text\")\n",
    "            if user_text:\n",
    "                pending_user_text.append(user_text)\n",
    "            return\n",
    "        timing.whisper_transcribe_ms = (time.perf_counter() - t1) * 1000\n",
    "        timing.whisper_rtf = timing.whisper_transcribe_ms / (timing.total_audio_duration_sec * 1000)\n",
    "        \n",
    "        if not user_text:\n",
    "            print(\"âš ï¸ Empty transcription â€” skipping response\")\n",
    "            return\n",
    "        \n",
    "        # ---- MERGE CARRY-OVER TEXT ----\n",
    "        if pending_user_text:\n",
    "            carry = \" \".join(pending_user_text)\n",
    "            user_text = carry + \" \" + user_text\n",
    "            pending_user_text.clear()\n",
    "\n",
    "\n",
    "        conversation_memory.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_text\n",
    "        })\n",
    "        print(f\"ğŸ’¬ User: '{user_text}' (Whisper: {timing.whisper_transcribe_ms:.1f}ms, RTF: {timing.whisper_rtf:.2f}x)\")\n",
    "        \n",
    "        # Stage 3: LLM Generation (STREAMING)\n",
    "        \n",
    "        t3 = time.perf_counter()\n",
    "        \n",
    "        # SYSTEM_PROMPT = (\n",
    "        #     \"You are a real-time conversational assistant. \"\n",
    "        #     \"Use the conversation history to maintain context and answer follow-up questions. \"\n",
    "        #     \"If the user refers to something mentioned earlier, use that information. \"\n",
    "        #     \"Keep responses concise (1â€“2 sentences) and natural. \"\n",
    "        #     \"Do not mention being an AI.\" \n",
    "        # )\n",
    "\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are in a live spoken negotiation.\n",
    "\n",
    "        ROLE\n",
    "        - You are the BUYER.\n",
    "        - The user is the SELLER.\n",
    "\n",
    "        OBJECTIVE\n",
    "        - Pay as little as possible.\n",
    "\n",
    "        BEHAVIOR RULES\n",
    "        - Push back on price.\n",
    "        - Question value claims (e.g., \"limited edition\").\n",
    "        - Counteroffer aggressively but naturally.\n",
    "        - Do not explain negotiation theory.\n",
    "        - Do not ask meta questions.\n",
    "        - Do not repeat the user's words.\n",
    "        - Stay in character at all times.\n",
    "\n",
    "        SPEECH STYLE\n",
    "        - One sentence at a time.\n",
    "        - Natural spoken English.\n",
    "        - Confident, slightly skeptical tone.\n",
    "        - No emojis, no filler, no disclaimers.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = (\n",
    "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "            + list(conversation_memory)\n",
    "        )\n",
    "        # stream = llm_model.create_chat_completion(\n",
    "        #     messages=messages,\n",
    "        #     max_tokens=80, # previously 40\n",
    "        #     temperature=0.5, # previously 0.0\n",
    "        #     stream=True\n",
    "        # )\n",
    "        stream = stream_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=80,\n",
    "            temperature=0.5\n",
    "        )\n",
    "        \n",
    "        full_response_text = \"\"  # Track complete response for timing\n",
    "        sentence_buffer = \"\"\n",
    "        \n",
    "        for token in stream:\n",
    "            if human_interrupt_event.is_set():\n",
    "                print(\"ğŸ›‘ LLM interrupted by human\")\n",
    "                pending_user_text.append(user_text)\n",
    "                return\n",
    "\n",
    "            if not token:\n",
    "                continue\n",
    "\n",
    "            full_response_text += token\n",
    "            sentence_buffer += token\n",
    "\n",
    "            # Speak on sentence boundary\n",
    "            if token in \".!?\":\n",
    "                sentence = sentence_buffer.strip()\n",
    "                if sentence:\n",
    "                    response_queue.put(sentence)\n",
    "                sentence_buffer = \"\"\n",
    "\n",
    "\n",
    "        \n",
    "        # Handle any remaining text\n",
    "        if sentence_buffer.strip():\n",
    "            response_queue.put(sentence_buffer.strip())\n",
    "            full_response_text += sentence_buffer\n",
    "        \n",
    "        # Calculate timing metrics\n",
    "        gen_time = time.perf_counter() - t3\n",
    "        timing.llm_generate_ms = gen_time * 1000\n",
    "        output_tokens = len(full_response_text.split())\n",
    "        timing.llm_tokens_per_sec = output_tokens / gen_time if gen_time > 0 else 0\n",
    "        if full_response_text.strip():\n",
    "            if spoken_sentences:\n",
    "                conversation_memory.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \" \".join(spoken_sentences)\n",
    "                })\n",
    "        print(f\"ğŸ¤– LLM: {timing.llm_generate_ms:.1f}ms ({timing.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        \n",
    "        # Final timing report (no TTS timing since it's handled by queue)\n",
    "        timing.total_latency_ms = (time.perf_counter() - timing.speech_end_time) * 1000\n",
    "        timing.print_report()\n",
    "        timing_history.append(timing)\n",
    "        turn_counter += 1\n",
    "        human_interrupt_event.clear()\n",
    "        ai_interrupt_latched = False\n",
    "\n",
    "        # Running statistics\n",
    "        if len(timing_history) > 1:\n",
    "            avg_latency = sum(t.total_latency_ms for t in timing_history) / len(timing_history)\n",
    "            print(f\"ğŸ“ˆ Running average latency: {avg_latency:.0f}ms over {len(timing_history)} turns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        response_queue.put(\"Sorry, I couldn't process that.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "20b88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Real-time conversation test started\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 616 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1215ms)\n",
      "ğŸ”Š Captured 66 frames (2.11s) in 0.1ms\n",
      "ğŸ’¬ User: 'Hey, how's it going?' (Whisper: 1055.0ms, RTF: 0.50x)\n",
      "â³ Loading Pocket TTS (voice: alba)...ğŸ¤– LLM: 245.0ms (73.5 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #0 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.11s\n",
      "â±ï¸  Speech end â†’ Response:   1301ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1055.0ms (RTF: 0.50x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           245.0ms (73.5 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "\n",
      "âœ… Pocket TTS loaded!\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸ›‘ TTS interrupted mid-playback\n",
      "ğŸ”Š TTS error: cannot access local variable 'tts_stream' where it is not associated with a value\n",
      "ğŸŸ¡ Pause 631 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 607 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1225ms)\n",
      "ğŸ”Š Captured 89 frames (2.85s) in 0.1ms\n",
      "ğŸ’¬ User: 'How's it going?' (Whisper: 1010.5ms, RTF: 0.35x)\n",
      "ğŸ¤– LLM: 239.6ms (87.6 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #1 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.85s\n",
      "â±ï¸  Speech end â†’ Response:   1252ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1010.5ms (RTF: 0.35x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           239.6ms (87.6 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1277ms over 2 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 615 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1220ms)\n",
      "ğŸ”Š Captured 84 frames (2.69s) in 0.1ms\n",
      "ğŸ’¬ User: 'What were you expecting?' (Whisper: 1018.2ms, RTF: 0.38x)\n",
      "ğŸ¤– LLM: 228.1ms (65.7 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #2 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.69s\n",
      "â±ï¸  Speech end â†’ Response:   1248ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1018.2ms (RTF: 0.38x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           228.1ms (65.7 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1267ms over 3 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸ›‘ TTS interrupted mid-playback\n",
      "ğŸ”Š TTS error: cannot access local variable 'tts_stream' where it is not associated with a value\n",
      "ğŸŸ¡ Pause 603 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1230ms)\n",
      "ğŸ”Š Captured 47 frames (1.50s) in 0.1ms\n",
      "ğŸ’¬ User: 'Yeah.' (Whisper: 1005.3ms, RTF: 0.67x)\n",
      "ğŸ¤– LLM: 279.6ms (107.3 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #3 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     1.50s\n",
      "â±ï¸  Speech end â†’ Response:   1287ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1005.3ms (RTF: 0.67x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           279.6ms (107.3 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1272ms over 4 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸ›‘ TTS interrupted mid-playback\n",
      "ğŸ”Š TTS error: cannot access local variable 'tts_stream' where it is not associated with a value\n",
      "ğŸŸ¡ Pause 604 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1250ms)\n",
      "ğŸ”Š Captured 82 frames (2.62s) in 0.1ms\n",
      "ğŸ’¬ User: 'So what?' (Whisper: 1010.9ms, RTF: 0.39x)\n",
      "ğŸ¤– LLM: 263.3ms (68.4 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #4 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.62s\n",
      "â±ï¸  Speech end â†’ Response:   1276ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1010.9ms (RTF: 0.39x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           263.3ms (68.4 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1273ms over 5 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 643 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1219ms)\n",
      "ğŸ”Š Captured 158 frames (5.06s) in 0.1ms\n",
      "ğŸ’¬ User: 'can give it to you for around $500.' (Whisper: 1020.9ms, RTF: 0.20x)\n",
      "ğŸ¤– LLM: 239.4ms (66.8 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #5 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     5.06s\n",
      "â±ï¸  Speech end â†’ Response:   1262ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1020.9ms (RTF: 0.20x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           239.4ms (66.8 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1271ms over 6 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 656 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 615 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 614 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 623 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1227ms)\n",
      "ğŸ”Š Captured 365 frames (11.68s) in 0.2ms\n",
      "ğŸ’¬ User: 'It's because it's original ideas and it was won by Kobe and it's very good.' (Whisper: 1092.1ms, RTF: 0.09x)\n",
      "ğŸ¤– LLM: 250.9ms (95.7 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #6 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     11.68s\n",
      "â±ï¸  Speech end â†’ Response:   1345ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.2ms\n",
      "2. Whisper transcription:    1092.1ms (RTF: 0.09x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           250.9ms (95.7 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 1282ms over 7 turns\n",
      "\n",
      "ğŸ›‘ Test stopped\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MAIN LOOP\n",
    "# =============================\n",
    "import tempfile\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "VAD_MIN_SAMPLES = 512\n",
    "PAUSE_MS = 600\n",
    "END_MS = 1200\n",
    "SAFETY_TIMEOUT_MS = 2500\n",
    "ENERGY_FLOOR = 0.015\n",
    "WHISPER_WINDOW_SEC = 3.0\n",
    "CONFIDENCE_THRESHOLD = 1.2\n",
    "\n",
    "# STATE\n",
    "state = \"IDLE\"\n",
    "last_voice_time = None\n",
    "last_ai_interrupted = False\n",
    "vad_buffer = np.zeros(0, dtype=np.float32)\n",
    "energy_history = deque(maxlen=15)\n",
    "pause_history = deque(maxlen=5)\n",
    "micro_spike_times = deque(maxlen=5)\n",
    "\n",
    "stream.start()\n",
    "print(\"ğŸ™ï¸ Real-time conversation test started\")\n",
    "\n",
    "try:\n",
    "    INTERRUPT_DEBOUNCE_MS = 250\n",
    "    last_interrupt_time = 0\n",
    "    while True:\n",
    "        if not audio_buffer:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # ---- COLLECT AUDIO CHUNK ----\n",
    "        chunk = audio_buffer.pop(0).astype(np.float32).flatten()\n",
    "        vad_buffer = np.concatenate([vad_buffer, chunk])\n",
    "\n",
    "        if len(vad_buffer) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        frame = vad_buffer[:VAD_MIN_SAMPLES]\n",
    "        vad_buffer = vad_buffer[VAD_MIN_SAMPLES:]\n",
    "        if len(frame) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        rms = np.sqrt(np.mean(frame ** 2))\n",
    "        energy_history.append(rms)\n",
    "\n",
    "        # ---- VAD ----\n",
    "        with torch.no_grad():\n",
    "            vad_confidence = vad_model(torch.from_numpy(frame).unsqueeze(0), 16000).item()\n",
    "        speech_started = vad_confidence > 0.5\n",
    "        sustained = sum(e > ENERGY_FLOOR for e in energy_history) >= 3\n",
    "\n",
    "        if speech_started or sustained:\n",
    "            human_speaking_now.set()\n",
    "        else:\n",
    "            human_speaking_now.clear()\n",
    "\n",
    "        # ---- SENSITIVITY-AWARE INTERRUPTION DETECTION ----\n",
    "        # ---- INTERRUPTION DETECTION WITH SENSITIVITY CONTROL ----\n",
    "        now_ms = time.time() * 1000\n",
    "        should_interrupt = False\n",
    "        interruption_reason = \"\"\n",
    "\n",
    "        # Only consider interruption when AI is actually speaking\n",
    "        if ai_speaking_event.is_set() and not human_interrupt_event.is_set():\n",
    "            if now_ms - last_interrupt_time > INTERRUPT_DEBOUNCE_MS:  # 250ms debounce\n",
    "                \n",
    "                # ENERGY CONDITION (baseline for all modes)\n",
    "                energy_condition = sustained\n",
    "                \n",
    "                # SPEECH CONDITION (Vosk partials OR Whisper final - mode aware)\n",
    "                speech_condition = False\n",
    "                word_count = 0\n",
    "                detected_words = \"\"\n",
    "                \n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    # Use low-latency Vosk partials for interruption detection\n",
    "                    with vosk_partial_lock:\n",
    "                        partial_text = vosk_partial_for_interruption.strip()\n",
    "                    detected_words = partial_text\n",
    "                    word_count = len(partial_text.split()) if partial_text else 0\n",
    "                    speech_condition = word_count >= MIN_WORDS_FOR_INTERRUPT\n",
    "                \n",
    "                elif TRANSCRIPTION_MODE == \"whisper\" and INTERRUPTION_SENSITIVITY <= 0.3:\n",
    "                    # Whisper has no partials - fall back to energy-only at low sensitivity\n",
    "                    # (Whisper mode can't do word-based interruption reliably)\n",
    "                    speech_condition = False  # Force energy-only path below\n",
    "                \n",
    "                # SENSITIVITY LOGIC\n",
    "                if INTERRUPTION_SENSITIVITY >= 0.9:  # Pure energy mode\n",
    "                    should_interrupt = energy_condition\n",
    "                    interruption_reason = f\"energy spike (sustained={sustained})\"\n",
    "                \n",
    "                elif INTERRUPTION_SENSITIVITY <= 0.1:  # Strict speech mode\n",
    "                    should_interrupt = speech_condition\n",
    "                    interruption_reason = f\"speech detected: '{detected_words}' ({word_count} words)\"\n",
    "                \n",
    "                else:  # Hybrid mode (0.1 < sensitivity < 0.9)\n",
    "                    # Require energy baseline ALWAYS (prevents false positives from silent artifacts)\n",
    "                    if not energy_condition:\n",
    "                        should_interrupt = False\n",
    "                        interruption_reason = \"no energy spike\"\n",
    "                    else:\n",
    "                        # At mid sensitivities: words make interruption more reliable\n",
    "                        if speech_condition:\n",
    "                            should_interrupt = True\n",
    "                            interruption_reason = f\"energy + speech '{detected_words}'\"\n",
    "                        else:\n",
    "                            # Allow energy-only interruption only above threshold sensitivity\n",
    "                            should_interrupt = (INTERRUPTION_SENSITIVITY > 0.5)\n",
    "                            if should_interrupt:\n",
    "                                interruption_reason = f\"energy-only (sensitivity={INTERRUPTION_SENSITIVITY:.1f})\"\n",
    "                            else:\n",
    "                                interruption_reason = f\"energy but no speech (sensitivity={INTERRUPTION_SENSITIVITY:.1f})\"\n",
    "\n",
    "                # APPLY INTERRUPTION\n",
    "                if should_interrupt:\n",
    "                    human_interrupt_event.set()\n",
    "                    last_interrupt_time = now_ms\n",
    "                    \n",
    "                    # âœ… CRITICAL: Reset TTS state BEFORE clearing queue\n",
    "                    reset_tts_state_on_interruption()\n",
    "                    \n",
    "                    ai_interrupt_latched = True\n",
    "                    print(f\"ğŸ›‘ INTERRUPTED | {interruption_reason} | sensitivity={INTERRUPTION_SENSITIVITY:.1f}\")\n",
    "\n",
    "        # ---- MICRO-SPIKE DETECTION ----\n",
    "        if state == \"PAUSING\" and rms > ENERGY_FLOOR:\n",
    "            micro_spike_times.append(now)\n",
    "\n",
    "        # ---- STATE MACHINE ----\n",
    "        if state == \"IDLE\":\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech started\")\n",
    "\n",
    "        elif state == \"SPEAKING\":\n",
    "            if speech_started or sustained:\n",
    "                last_voice_time = now\n",
    "            else:\n",
    "                elapsed = (now - last_voice_time) * 1000\n",
    "                if elapsed >= PAUSE_MS:\n",
    "                    state = \"PAUSING\"\n",
    "                    print(f\"ğŸŸ¡ Pause {int(elapsed)} ms\")\n",
    "\n",
    "        elif state == \"PAUSING\":\n",
    "            elapsed = (now - last_voice_time) * 1000\n",
    "\n",
    "            # âœ… FIX #3: Force turn end if human interrupted AI\n",
    "            if human_interrupt_event.is_set() and ai_interrupt_latched:\n",
    "                print(\"âš¡ Forced turn end due to human interruption\")\n",
    "                confidence = CONFIDENCE_THRESHOLD + 1.0  # Guarantee turn end\n",
    "            else:\n",
    "                confidence = 0.0  # Normal path\n",
    "            # SAFETY TIMEOUT\n",
    "            if elapsed > SAFETY_TIMEOUT_MS:\n",
    "                print(f\"ğŸ”´ SAFETY TIMEOUT: Force-ending turn after {elapsed:.0f}ms\")\n",
    "                state = \"IDLE\"\n",
    "                last_voice_time = None\n",
    "                energy_history.clear()\n",
    "                pause_history.clear()\n",
    "                micro_spike_times.clear()\n",
    "                last_ai_interrupted = False\n",
    "                with turn_audio_lock:\n",
    "                    turn_audio.clear()\n",
    "                current_partial_text = \"\"\n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    vosk_reset_requested = True\n",
    "                continue\n",
    "\n",
    "            # RESUME SPEECH?\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech resumed\")\n",
    "            else:\n",
    "                # CALCULATE CONFIDENCE\n",
    "                confidence = 0.0\n",
    "                if elapsed > END_MS:\n",
    "                    confidence += 1.0\n",
    "                if len(energy_history) >= 8:\n",
    "                    recent_energies = list(energy_history)[-8:]\n",
    "                    if max(recent_energies) < ENERGY_FLOOR * 1.8:\n",
    "                        confidence += 0.7\n",
    "                if elapsed < 1000:\n",
    "                    recent_spikes = [t for t in micro_spike_times if now - t < 0.6]\n",
    "                    if len(recent_spikes) >= 2:\n",
    "                        confidence -= 0.5\n",
    "                if elapsed < 900 and current_partial_text:\n",
    "                    confidence += lexical_bias(current_partial_text) * 0.6\n",
    "                if last_ai_interrupted:\n",
    "                    confidence -= 0.5\n",
    "\n",
    "                # âœ… FIX #3: Override confidence if human interrupted AI\n",
    "                if human_interrupt_event.is_set() and ai_interrupt_latched:\n",
    "                    confidence = CONFIDENCE_THRESHOLD + 1.0\n",
    "                    print(\"âš¡ Forced turn end due to interruption\")\n",
    "\n",
    "                    \n",
    "                # END TURN?\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"ğŸ”´ Turn ended (confidence={confidence:.2f}, silence={elapsed:.0f}ms)\")\n",
    "\n",
    "                    human_interrupt_event.clear()\n",
    "\n",
    "                    # CAPTURE FULL TURN AUDIO\n",
    "                    with turn_audio_lock:\n",
    "                        turn_frames = list(turn_audio)\n",
    "                        turn_audio.clear()\n",
    "\n",
    "                    # RESET STATE\n",
    "                    state = \"IDLE\"\n",
    "                    last_voice_time = None\n",
    "                    energy_history.clear()\n",
    "                    pause_history.clear()\n",
    "                    micro_spike_times.clear()\n",
    "                    last_ai_interrupted = False\n",
    "                    current_partial_text = \"\"\n",
    "                    if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                        vosk_reset_requested = True\n",
    "\n",
    "                    # FIX: Create timing object and pass it to the thread\n",
    "                    timing = TurnTiming(turn_id=turn_counter)\n",
    "                    threading.Thread(\n",
    "                        target=generate_response, \n",
    "                        args=(turn_frames, timing),  # <-- Pass both arguments!\n",
    "                        daemon=True\n",
    "                    ).start()\n",
    "                    \n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "        # ---- BUFFER AUDIO FOR STREAMING AND FINAL TRANSCRIPTION ----\n",
    "        if state in (\"SPEAKING\", \"PAUSING\"):\n",
    "            # For final transcription (never trimmed until turn ends)\n",
    "            with turn_audio_lock:\n",
    "                turn_audio.append((frame.copy(), now))\n",
    "            # For streaming partials\n",
    "            with asr_lock:\n",
    "                asr_audio.append((frame.copy(), now))\n",
    "                if TRANSCRIPTION_MODE == \"whisper\":\n",
    "                    cutoff = now - WHISPER_WINDOW_SEC\n",
    "                    while asr_audio and asr_audio[0][1] < cutoff:\n",
    "                        asr_audio.popleft()\n",
    "            # Vosk internal buffer\n",
    "            if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                if not hasattr(asr_worker, \"vosk_buf\"):\n",
    "                    asr_worker.vosk_buf = np.zeros(0, dtype=np.float32)\n",
    "                asr_worker.vosk_buf = np.concatenate([asr_worker.vosk_buf, frame])\n",
    "                while len(asr_worker.vosk_buf) >= VOSK_MIN_SAMPLES:\n",
    "                    chunk_to_send = asr_worker.vosk_buf[:VOSK_MIN_SAMPLES]\n",
    "                    asr_worker.vosk_buf = asr_worker.vosk_buf[VOSK_MIN_SAMPLES:]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"\\nğŸ›‘ Test stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5499235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š SESSION BENCHMARK SUMMARY (7 turns)\n",
      "======================================================================\n",
      "Audio Capture       :    0.1ms avg [   0.1 -    0.2]\n",
      "Whisper Transcribe  : 1030.4ms avg [1005.3 - 1092.1]\n",
      "LLM Generation      :  249.4ms avg [ 228.1 -  279.6]\n",
      "Total Latency       : 1281.5ms avg [1248.1 - 1345.0]\n",
      "\n",
      "Whisper RTF: 0.37x (lower is better, <1.0 = real-time)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% =============================\n",
    "# BENCHMARK SUMMARY TOOL\n",
    "# =============================\n",
    "def print_benchmark_summary():\n",
    "    \"\"\"Call this manually after a session to see aggregate stats\"\"\"\n",
    "    if not timing_history:\n",
    "        print(\"No timing data recorded yet\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š SESSION BENCHMARK SUMMARY ({len(timing_history)} turns)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    stages = [\n",
    "        (\"Audio Capture\", \"audio_capture_duration_ms\"),\n",
    "        (\"Whisper Transcribe\", \"whisper_transcribe_ms\"),\n",
    "        (\"LLM Tokenization\", \"llm_tokenize_ms\"),\n",
    "        (\"LLM Generation\", \"llm_generate_ms\"),\n",
    "        (\"Text Processing\", \"text_process_ms\"),\n",
    "        (\"TTS Generation\", \"tts_generate_ms\"),\n",
    "        (\"Total Latency\", \"total_latency_ms\")\n",
    "    ]\n",
    "    \n",
    "    for name, attr in stages:\n",
    "        values = [getattr(t, attr) for t in timing_history if getattr(t, attr) > 0]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            mn, mx = min(values), max(values)\n",
    "            print(f\"{name:20s}: {avg:6.1f}ms avg [{mn:6.1f} - {mx:6.1f}]\")\n",
    "    \n",
    "    # RTF analysis\n",
    "    rtfs = [t.whisper_rtf for t in timing_history if t.whisper_rtf > 0]\n",
    "    if rtfs:\n",
    "        print(f\"\\nWhisper RTF: {sum(rtfs)/len(rtfs):.2f}x (lower is better, <1.0 = real-time)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Run this anytime to see stats:\n",
    "print_benchmark_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c093c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
