{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2b0a5605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.autograd.grad_mode.set_grad_enabled(mode=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch_bootstrap.py\n",
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "import torch\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(1)\n",
    "torch.set_grad_enabled(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f817fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from collections import deque\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\"  # Only show errors\n",
    "from llama_cpp import Llama\n",
    "\n",
    "try:\n",
    "    from pocket_tts import TTSModel\n",
    "    POCKET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POCKET_AVAILABLE = False\n",
    "    print(\"âš ï¸ pocket-tts not installed. Install with: uv add pocket-tts\")\n",
    "\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "TRANSCRIPTION_MODE = \"vosk\"  # Options: \"vosk\" (fast partials) or \"whisper\"\n",
    "PROJECT_ROOT = r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\"\n",
    "TTS_MODE = \"pocket\"  # Options: \"pocket\" (neural) or \"powershell\" (system)\n",
    "POCKET_VOICE = \"alba\"  # Options: alba, marius, javert, jean, fantine, cosette, \n",
    "\n",
    "# =============================\n",
    "# GLOBAL INTERRUPTION CONTROL\n",
    "# =============================\n",
    "human_interrupt_event = threading.Event()   # Human started speaking\n",
    "ai_speaking_event = threading.Event()       # AI currently speaking\n",
    "processing_lock = threading.Lock()           # Guards LLM/Whisper sections\n",
    "human_speaking_now = threading.Event()\n",
    "ai_interrupt_latched = False\n",
    "spoken_sentences = []\n",
    "tts_playback_lock = threading.Lock()\n",
    "\n",
    "\n",
    "# Carry-over buffer when human interrupts mid-processing\n",
    "pending_user_text = deque()  # list[str]\n",
    "\n",
    "# =============================\n",
    "# EPHEMERAL CONVERSATION MEMORY\n",
    "# =============================\n",
    "MAX_MEMORY_TURNS = 24  # 3 user + 3 assistant turns (tune later)\n",
    "\n",
    "conversation_memory = deque(maxlen=MAX_MEMORY_TURNS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "25a1a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PC/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (for final transcription)...\n",
      "Loading Vosk...\n",
      "ASR mode: vosk\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# LOAD MODELS (ALWAYS LOAD WHISPER FOR FINAL TRANSCRIPTION)\n",
    "# =============================\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "\n",
    "print(\"Loading Whisper (for final transcription)...\")\n",
    "whisper = WhisperModel(\n",
    "    r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\whisper\\distil-small.en\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\",\n",
    "    local_files_only=True,  # Force local, no hub download\n",
    "    cpu_threads=8\n",
    ")\n",
    "\n",
    "# Load Vosk only if needed\n",
    "vosk_model = None\n",
    "vosk_rec = None\n",
    "if TRANSCRIPTION_MODE == \"vosk\":\n",
    "    from vosk import Model, KaldiRecognizer\n",
    "    print(\"Loading Vosk...\")\n",
    "    vosk_model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    vosk_rec = KaldiRecognizer(vosk_model, 16000)\n",
    "    vosk_rec.SetWords(True)\n",
    "\n",
    "print(f\"ASR mode: {TRANSCRIPTION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c6b0f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking model path: D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\llm\\qwen2.5-3b-instruct-q5_k_m.gguf\n",
      "   Exists? True\n",
      "   Size: 2.27 GB\n"
     ]
    }
   ],
   "source": [
    "# GLOBALS (replace old _llm_model/_llm_tokenizer)\n",
    "_llama_model = None\n",
    "GGUF_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"llm\" ,\"qwen2.5-3b-instruct-q5_k_m.gguf\")  # Adjust path as needed\n",
    "\n",
    "def get_llm():\n",
    "    global _llama_model\n",
    "    if _llama_model is None:\n",
    "        print(\"â³ Loading Qwen2.5-3B (Q5_K_M GGUF) on CPU...\")\n",
    "        try:\n",
    "            _llama_model = Llama(\n",
    "                model_path=GGUF_MODEL_PATH,\n",
    "                n_ctx=2048,\n",
    "                n_threads=8,\n",
    "                n_threads_batch=8,\n",
    "                n_batch=512,\n",
    "                n_gqa=1,  # âš ï¸ CRITICAL FOR QWEN\n",
    "                verbose=False,  # Enable loading logs\n",
    "                use_mmap=True,\n",
    "                use_mlock=False,\n",
    "                rope_freq_base=1000000.0\n",
    "            )\n",
    "            print(\"âœ… Qwen2.5-3B loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ FAILED to load GGUF model: {e}\")\n",
    "            print(f\"   Check if file exists: {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "            raise  # Force crash to see error\n",
    "    return _llama_model\n",
    "\n",
    "# Add after defining GGUF_MODEL_PATH\n",
    "print(f\"ğŸ” Checking model path: {GGUF_MODEL_PATH}\")\n",
    "print(f\"   Exists? {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "print(f\"   Size: {os.path.getsize(GGUF_MODEL_PATH) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "089cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR worker started\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# AUDIO SETUP\n",
    "# =============================\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = []\n",
    "VOSK_MIN_SAMPLES = 3200  # 0.2 sec @ 16kHz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_buffer.append(indata.copy())\n",
    "\n",
    "stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ASR WORKER (STREAMING PARTIALS)\n",
    "# =============================\n",
    "asr_audio = deque()      # For streaming partials (trimmed)\n",
    "turn_audio = deque()     # For final transcription (full turn)\n",
    "asr_lock = threading.Lock()\n",
    "turn_audio_lock = threading.Lock()\n",
    "current_partial_text = \"\"\n",
    "vosk_reset_requested = False\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def asr_worker():\n",
    "    global current_partial_text, vosk_reset_requested\n",
    "    WHISPER_WINDOW_SEC = 1.2\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05 if TRANSCRIPTION_MODE == \"vosk\" else 0.7)\n",
    "\n",
    "        if TRANSCRIPTION_MODE == \"whisper\":\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                now = time.time()\n",
    "                recent = [frame for frame, t in asr_audio if now - t <= WHISPER_WINDOW_SEC]\n",
    "            if not recent:\n",
    "                continue\n",
    "            audio_np = np.concatenate(recent)\n",
    "            segments, _ = whisper.transcribe(\n",
    "                audio_np, language=\"en\", vad_filter=False, beam_size=1, temperature=0.0\n",
    "            )\n",
    "            text = \" \".join(seg.text for seg in segments).strip()\n",
    "            if text and text != current_partial_text:\n",
    "                current_partial_text = text\n",
    "                print(\"ğŸ“ Partial:\", text)\n",
    "\n",
    "        else:  # Vosk mode\n",
    "            if vosk_reset_requested:\n",
    "                vosk_rec.Reset()\n",
    "                vosk_reset_requested = False\n",
    "                current_partial_text = \"\"\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                frame, _ = asr_audio.popleft()\n",
    "                if len(frame) < VOSK_MIN_SAMPLES:\n",
    "                    continue\n",
    "                pcm16 = float32_to_int16(frame)\n",
    "            try:\n",
    "                if vosk_rec.AcceptWaveform(pcm16.tobytes()):\n",
    "                    res = json.loads(vosk_rec.Result())\n",
    "                    text = res.get(\"text\", \"\").strip()\n",
    "                    if text:\n",
    "                        print(\"ğŸ“ Final:\", text)\n",
    "                        current_partial_text = \"\"\n",
    "                else:\n",
    "                    res = json.loads(vosk_rec.PartialResult())\n",
    "                    partial = res.get(\"partial\", \"\").strip()\n",
    "                    if partial and partial != current_partial_text:\n",
    "                        current_partial_text = partial\n",
    "                        print(\"ğŸ“ Partial:\", partial)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "threading.Thread(target=asr_worker, daemon=True).start()\n",
    "print(\"ASR worker started\")\n",
    "\n",
    "# =============================\n",
    "# TURN-TAKING RULES\n",
    "# =============================\n",
    "TRAILING_CONJUNCTIONS = {\"and\",\"or\",\"but\",\"because\",\"so\",\"that\",\"which\",\"who\",\"when\",\"if\",\"though\",\"while\"}\n",
    "OPEN_ENDED_PREFIXES = (\"i think\",\"i guess\",\"i'm not sure\",\"the thing is\",\"it depends\")\n",
    "QUESTION_LEADINS = (\"do you think\",\"would you say\",\"is it possible\",\"can you\")\n",
    "SELF_REPAIR_MARKERS = (\"i mean\",\"actually\",\"sorry\",\"no wait\")\n",
    "FILLER_ENDINGS = (\"uh\",\"um\",\"like\",\"you know\",\"kind of\")\n",
    "\n",
    "def lexical_bias(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    t = text.lower().strip()\n",
    "    words = t.split()\n",
    "    score = 0.0\n",
    "    if words[-1] in TRAILING_CONJUNCTIONS: score -= 1.0\n",
    "    if any(t.startswith(p) for p in OPEN_ENDED_PREFIXES): score -= 0.6\n",
    "    if any(t.startswith(q) for q in QUESTION_LEADINS): score -= 0.5\n",
    "    if any(m in t[-20:] for m in SELF_REPAIR_MARKERS): score -= 0.4\n",
    "    if words[-1] in FILLER_ENDINGS: score -= 0.7\n",
    "    return score\n",
    "\n",
    "def energy_decay_score(energy_history):\n",
    "    if len(energy_history) < 5: return 0.0\n",
    "    x = np.arange(len(energy_history))\n",
    "    y = np.array(energy_history)\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    return 0.8 if slope < -0.00015 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a017d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# POCKET TTS LOADING\n",
    "# =============================\n",
    "_pocket_model = None\n",
    "_pocket_voice_state = None\n",
    "_pocket_sample_rate = 24000  # Pocket TTS default\n",
    "\n",
    "def get_pocket_tts():\n",
    "    \"\"\"Lazy load Pocket TTS model\"\"\"\n",
    "    global _pocket_model, _pocket_voice_state, _pocket_sample_rate\n",
    "    if _pocket_model is None:\n",
    "        if not POCKET_AVAILABLE:\n",
    "            raise ImportError(\"pocket-tts not installed\")\n",
    "        print(f\"â³ Loading Pocket TTS (voice: {POCKET_VOICE})...\")\n",
    "        _pocket_model = TTSModel.load_model()\n",
    "        _pocket_voice_state = _pocket_model.get_state_for_audio_prompt(POCKET_VOICE)\n",
    "        _pocket_sample_rate = _pocket_model.sample_rate\n",
    "        print(\"âœ… Pocket TTS loaded!\")\n",
    "    return _pocket_model, _pocket_voice_state, _pocket_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f5c3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def speak(text):\n",
    "    if not text or not text.strip():\n",
    "        return\n",
    "\n",
    "    with tts_playback_lock:\n",
    "        ai_speaking_event.set()\n",
    "\n",
    "        try:\n",
    "            if TTS_MODE == \"pocket\":\n",
    "                model, voice_state, sr = get_pocket_tts()\n",
    "                audio = model.generate_audio(voice_state, text)\n",
    "                audio_np = audio.numpy() if hasattr(audio, 'numpy') else np.array(audio)\n",
    "\n",
    "                sd.play(audio_np, sr)\n",
    "                sd.wait()  # ğŸ”’ HARD BLOCK until playback ends\n",
    "\n",
    "            else:\n",
    "                speak_powershell(text)\n",
    "\n",
    "        finally:\n",
    "            ai_speaking_event.clear()\n",
    "\n",
    "\n",
    "\n",
    "def speak_powershell(text):\n",
    "    \"\"\"Original Windows PowerShell TTS (fallback)\"\"\"\n",
    "    safe_text = text.replace('\"', '\"\"').replace('\\n', ' ').replace('\\r', '')\n",
    "    cmd = f'Add-Type -AssemblyName System.Speech; $s=New-Object System.Speech.Synthesis.SpeechSynthesizer; $s.Speak(\"{safe_text}\")'\n",
    "    try:\n",
    "        subprocess.run([\"powershell\", \"-Command\", cmd],\n",
    "                       stdout=subprocess.DEVNULL,\n",
    "                       stderr=subprocess.DEVNULL,\n",
    "                       timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”Š Speech error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d8cb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# WINDOWS-RELIABLE TTS (POWER SHELL)\n",
    "# =============================\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def tts_main_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            text = response_queue.get(timeout=0.1)\n",
    "\n",
    "            # If human spoke, drop all pending AI speech\n",
    "            if human_interrupt_event.is_set():\n",
    "                with response_queue.mutex:\n",
    "                    response_queue.queue.clear()\n",
    "                continue\n",
    "\n",
    "            speak(text)\n",
    "\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "\n",
    "# Started earlier in your code:\n",
    "threading.Thread(target=tts_main_loop, daemon=False).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e1d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the TOP of your notebook (before main loop), make sure these exist:\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class TurnTiming:\n",
    "    turn_id: int = 0\n",
    "    speech_end_time: float = 0.0\n",
    "    audio_capture_duration_ms: float = 0.0\n",
    "    whisper_transcribe_ms: float = 0.0\n",
    "    whisper_rtf: float = 0.0\n",
    "    llm_tokenize_ms: float = 0.0\n",
    "    llm_generate_ms: float = 0.0\n",
    "    llm_tokens_per_sec: float = 0.0\n",
    "    text_process_ms: float = 0.0\n",
    "    tts_generate_ms: float = 0.0\n",
    "    tts_playback_ms: float = 0.0\n",
    "    total_latency_ms: float = 0.0\n",
    "    total_audio_duration_sec: float = 0.0\n",
    "    \n",
    "    def print_report(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“Š TURN #{self.turn_id} TIMING AUDIT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ğŸ™ï¸  User audio duration:     {self.total_audio_duration_sec:.2f}s\")\n",
    "        print(f\"â±ï¸  Speech end â†’ Response:   {self.total_latency_ms:.0f}ms total\")\n",
    "        print(f\"{'â”€'*40}\")\n",
    "        print(f\"1. Audio buffer capture:     {self.audio_capture_duration_ms:.1f}ms\")\n",
    "        print(f\"2. Whisper transcription:    {self.whisper_transcribe_ms:.1f}ms (RTF: {self.whisper_rtf:.2f}x)\")\n",
    "        print(f\"3. LLM tokenization:         {self.llm_tokenize_ms:.1f}ms\")\n",
    "        print(f\"4. LLM generation:           {self.llm_generate_ms:.1f}ms ({self.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        print(f\"5. Text processing:          {self.text_process_ms:.1f}ms\")\n",
    "        if self.tts_generate_ms > 0:\n",
    "            print(f\"6. TTS generation:           {self.tts_generate_ms:.1f}ms\")\n",
    "            print(f\"7. Audio playback:           {self.tts_playback_ms:.1f}ms\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "turn_counter = 0\n",
    "timing_history: List[TurnTiming] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8931e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(frames, timing: TurnTiming):\n",
    "    global turn_counter\n",
    "    with processing_lock:\n",
    "        timing.speech_end_time = time.perf_counter()\n",
    "    \n",
    "    # Skip only if human is actively speaking NOW\n",
    "        if human_speaking_now.is_set():\n",
    "            print(\"âš ï¸ Skipping generation â€” human speaking\")\n",
    "            return\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Audio Capture\n",
    "        t0 = time.perf_counter()\n",
    "        if not frames:\n",
    "            print(\"âš ï¸ No audio captured â€” skipping response\")\n",
    "            return\n",
    "        \n",
    "        full_audio = np.concatenate([frame for frame, _ in frames])\n",
    "        timing.total_audio_duration_sec = full_audio.shape[0] / 16000.0\n",
    "        timing.audio_capture_duration_ms = (time.perf_counter() - t0) * 1000\n",
    "        print(f\"ğŸ”Š Captured {len(frames)} frames ({timing.total_audio_duration_sec:.2f}s) in {timing.audio_capture_duration_ms:.1f}ms\")\n",
    "        \n",
    "        # Stage 2: Whisper Transcription\n",
    "        t1 = time.perf_counter()\n",
    "        segments, info = whisper.transcribe(\n",
    "            full_audio,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            condition_on_previous_text=False\n",
    "        )\n",
    "        user_text = \" \".join(seg.text for seg in segments).strip()\n",
    "        if human_interrupt_event.is_set():\n",
    "            print(\"ğŸ§  Interrupted during transcription, buffering text\")\n",
    "            if user_text:\n",
    "                pending_user_text.append(user_text)\n",
    "            return\n",
    "        timing.whisper_transcribe_ms = (time.perf_counter() - t1) * 1000\n",
    "        timing.whisper_rtf = timing.whisper_transcribe_ms / (timing.total_audio_duration_sec * 1000)\n",
    "        \n",
    "        if not user_text:\n",
    "            print(\"âš ï¸ Empty transcription â€” skipping response\")\n",
    "            return\n",
    "        \n",
    "        # ---- MERGE CARRY-OVER TEXT ----\n",
    "        if pending_user_text:\n",
    "            carry = \" \".join(pending_user_text)\n",
    "            user_text = carry + \" \" + user_text\n",
    "            pending_user_text.clear()\n",
    "\n",
    "\n",
    "        conversation_memory.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_text\n",
    "        })\n",
    "        print(f\"ğŸ’¬ User: '{user_text}' (Whisper: {timing.whisper_transcribe_ms:.1f}ms, RTF: {timing.whisper_rtf:.2f}x)\")\n",
    "        \n",
    "        # Stage 3: LLM Generation (STREAMING)\n",
    "        llm_model = get_llm()\n",
    "        t3 = time.perf_counter()\n",
    "        \n",
    "        # SYSTEM_PROMPT = (\n",
    "        #     \"You are a real-time conversational assistant. \"\n",
    "        #     \"Use the conversation history to maintain context and answer follow-up questions. \"\n",
    "        #     \"If the user refers to something mentioned earlier, use that information. \"\n",
    "        #     \"Keep responses concise (1â€“2 sentences) and natural. \"\n",
    "        #     \"Do not mention being an AI.\" \n",
    "        # )\n",
    "\n",
    "\n",
    "        SYSTEM_PROMPT = \"\"\"\n",
    "        You are in a live spoken negotiation.\n",
    "\n",
    "        ROLE\n",
    "        - You are the BUYER.\n",
    "        - The user is the SELLER.\n",
    "\n",
    "        OBJECTIVE\n",
    "        - Pay as little as possible.\n",
    "\n",
    "        BEHAVIOR RULES\n",
    "        - Push back on price.\n",
    "        - Question value claims (e.g., \"limited edition\").\n",
    "        - Counteroffer aggressively but naturally.\n",
    "        - Do not explain negotiation theory.\n",
    "        - Do not ask meta questions.\n",
    "        - Do not repeat the user's words.\n",
    "        - Stay in character at all times.\n",
    "\n",
    "        SPEECH STYLE\n",
    "        - One sentence at a time.\n",
    "        - Natural spoken English.\n",
    "        - Confident, slightly skeptical tone.\n",
    "        - No emojis, no filler, no disclaimers.\n",
    "        \"\"\"\n",
    "\n",
    "        messages = (\n",
    "            [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
    "            + list(conversation_memory)\n",
    "        )\n",
    "        stream = llm_model.create_chat_completion(\n",
    "            messages=messages,\n",
    "            max_tokens=80, # previously 40\n",
    "            temperature=0.5, # previously 0.0\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response_text = \"\"  # Track complete response for timing\n",
    "        sentence_buffer = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if human_interrupt_event.is_set():\n",
    "                print(\"ğŸ›‘ LLM interrupted by human\")\n",
    "                pending_user_text.append(user_text)\n",
    "                return\n",
    "            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                if \"content\" in delta:\n",
    "                    token = delta[\"content\"]\n",
    "                    full_response_text += token\n",
    "                    sentence_buffer += token\n",
    "                    \n",
    "                    # Speak complete sentences immediately\n",
    "                    if token in \".!?\":\n",
    "                        sentence = sentence_buffer.strip()\n",
    "                        if sentence:\n",
    "                            response_queue.put(sentence)\n",
    "                        sentence_buffer = \"\"\n",
    "        \n",
    "        # Handle any remaining text\n",
    "        if sentence_buffer.strip():\n",
    "            response_queue.put(sentence_buffer.strip())\n",
    "            full_response_text += sentence_buffer\n",
    "        \n",
    "        # Calculate timing metrics\n",
    "        gen_time = time.perf_counter() - t3\n",
    "        timing.llm_generate_ms = gen_time * 1000\n",
    "        output_tokens = len(full_response_text.split())\n",
    "        timing.llm_tokens_per_sec = output_tokens / gen_time if gen_time > 0 else 0\n",
    "        if full_response_text.strip():\n",
    "            if spoken_sentences:\n",
    "                conversation_memory.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \" \".join(spoken_sentences)\n",
    "                })\n",
    "        print(f\"ğŸ¤– LLM: {timing.llm_generate_ms:.1f}ms ({timing.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        \n",
    "        # Final timing report (no TTS timing since it's handled by queue)\n",
    "        timing.total_latency_ms = (time.perf_counter() - timing.speech_end_time) * 1000\n",
    "        timing.print_report()\n",
    "        timing_history.append(timing)\n",
    "        turn_counter += 1\n",
    "        \n",
    "        # Running statistics\n",
    "        if len(timing_history) > 1:\n",
    "            avg_latency = sum(t.total_latency_ms for t in timing_history) / len(timing_history)\n",
    "            print(f\"ğŸ“ˆ Running average latency: {avg_latency:.0f}ms over {len(timing_history)} turns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        response_queue.put(\"Sorry, I couldn't process that.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20b88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Real-time conversation test started\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 601 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1224ms)\n",
      "ğŸ”Š Captured 64 frames (2.05s) in 0.1ms\n",
      "ğŸ’¬ User: 'Hey, how's it?' (Whisper: 1125.2ms, RTF: 0.55x)\n",
      "ğŸ¤– LLM: 9493.0ms (0.4 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #15 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.05s\n",
      "â±ï¸  Speech end â†’ Response:   10620ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1125.2ms (RTF: 0.55x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           9493.0ms (0.4 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 2988ms over 16 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 604 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1202ms)\n",
      "ğŸ”Š Captured 110 frames (3.52s) in 0.1ms\n",
      "ğŸ’¬ User: 'This is good, do we wanna buy something?' (Whisper: 1139.4ms, RTF: 0.32x)\n",
      "ğŸ¤– LLM: 1838.2ms (8.7 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #16 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     3.52s\n",
      "â±ï¸  Speech end â†’ Response:   2979ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1139.4ms (RTF: 0.32x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1838.2ms (8.7 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 2988ms over 17 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 649 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1206ms)\n",
      "ğŸ”Š Captured 175 frames (5.60s) in 0.2ms\n",
      "ğŸ’¬ User: 'Yeah, it's a limited edition pen and I want to sell it for $15.' (Whisper: 1168.3ms, RTF: 0.21x)\n",
      "ğŸ¤– LLM: 1624.5ms (7.4 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #17 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     5.60s\n",
      "â±ï¸  Speech end â†’ Response:   2796ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.2ms\n",
      "2. Whisper transcription:    1168.3ms (RTF: 0.21x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1624.5ms (7.4 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 2977ms over 18 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 629 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1203ms)\n",
      "ğŸ”Š Captured 237 frames (7.58s) in 0.1ms\n",
      "ğŸ’¬ User: 'why don't why do you think it's not worth that much I did tell you that it was a  limited edition' (Whisper: 1209.9ms, RTF: 0.16x)\n",
      "ğŸ¤– LLM: 2167.2ms (6.9 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #18 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     7.58s\n",
      "â±ï¸  Speech end â†’ Response:   3379ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1209.9ms (RTF: 0.16x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           2167.2ms (6.9 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 2998ms over 19 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 630 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 627 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1203ms)\n",
      "ğŸ”Š Captured 469 frames (15.01s) in 0.2ms\n",
      "ğŸ’¬ User: 'But I think that if you take into account that it's a fountain pain and you can write  it upside down, then you should consider like paying $15 for it.' (Whisper: 1302.1ms, RTF: 0.09x)\n",
      "ğŸ¤– LLM: 3202.1ms (8.7 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #19 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     15.01s\n",
      "â±ï¸  Speech end â†’ Response:   4506ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.2ms\n",
      "2. Whisper transcription:    1302.1ms (RTF: 0.09x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           3202.1ms (8.7 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3073ms over 20 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 626 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1213ms)\n",
      "ğŸ”Š Captured 173 frames (5.54s) in 0.1ms\n",
      "ğŸ’¬ User: 'Is there anything I can do to convince you that it's $15 worth?' (Whisper: 1174.2ms, RTF: 0.21x)\n",
      "ğŸ¤– LLM: 1709.4ms (7.6 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #20 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     5.54s\n",
      "â±ï¸  Speech end â†’ Response:   2887ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1174.2ms (RTF: 0.21x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1709.4ms (7.6 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3065ms over 21 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 607 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1258ms)\n",
      "ğŸ”Š Captured 196 frames (6.27s) in 0.1ms\n",
      "ğŸ’¬ User: 'I don't think so. This pen was actually used by Robert Downey Jr.' (Whisper: 1160.7ms, RTF: 0.19x)\n",
      "ğŸ¤– LLM: 1892.9ms (6.3 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #21 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     6.27s\n",
      "â±ï¸  Speech end â†’ Response:   3056ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1160.7ms (RTF: 0.19x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1892.9ms (6.3 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3064ms over 22 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 608 ms\n",
      "ğŸŸ¢ Speech resumed\n",
      "ğŸŸ¡ Pause 607 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1205ms)\n",
      "ğŸ”Š Captured 123 frames (3.94s) in 0.1ms\n",
      "ğŸ’¬ User: 'Can you settle for 14?' (Whisper: 1127.1ms, RTF: 0.29x)\n",
      "ğŸ¤– LLM: 1997.7ms (9.0 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #22 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     3.94s\n",
      "â±ï¸  Speech end â†’ Response:   3127ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1127.1ms (RTF: 0.29x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1997.7ms (9.0 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3067ms over 23 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 629 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1216ms)\n",
      "ğŸ”Š Captured 57 frames (1.82s) in 0.1ms\n",
      "ğŸ’¬ User: 'What do you mean?' (Whisper: 1098.9ms, RTF: 0.60x)\n",
      "ğŸ¤– LLM: 1163.2ms (10.3 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #23 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     1.82s\n",
      "â±ï¸  Speech end â†’ Response:   2264ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1098.9ms (RTF: 0.60x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1163.2ms (10.3 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3033ms over 24 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 619 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1218ms)\n",
      "ğŸ”Š Captured 125 frames (4.00s) in 0.1ms\n",
      "ğŸ’¬ User: '14 dollar sounds doable' (Whisper: 1135.5ms, RTF: 0.28x)\n",
      "ğŸ¤– LLM: 9917.4ms (0.8 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #24 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     4.00s\n",
      "â±ï¸  Speech end â†’ Response:   11055ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1135.5ms (RTF: 0.28x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           9917.4ms (0.8 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3354ms over 25 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 619 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1204ms)\n",
      "ğŸ”Š Captured 157 frames (5.02s) in 0.1ms\n",
      "ğŸ’¬ User: 'What? You are not giving it to me? I am giving it to you, remember?' (Whisper: 1198.7ms, RTF: 0.24x)\n",
      "ğŸ¤– LLM: 10847.0ms (1.9 tok/s)\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #25 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     5.02s\n",
      "â±ï¸  Speech end â†’ Response:   12048ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1198.7ms (RTF: 0.24x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           10847.0ms (1.9 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 3689ms over 26 turns\n",
      "\n",
      "ğŸ›‘ Test stopped\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MAIN LOOP\n",
    "# =============================\n",
    "import tempfile\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "VAD_MIN_SAMPLES = 512\n",
    "PAUSE_MS = 600\n",
    "END_MS = 1200\n",
    "SAFETY_TIMEOUT_MS = 2500\n",
    "ENERGY_FLOOR = 0.015\n",
    "WHISPER_WINDOW_SEC = 3.0\n",
    "CONFIDENCE_THRESHOLD = 1.2\n",
    "\n",
    "# STATE\n",
    "state = \"IDLE\"\n",
    "last_voice_time = None\n",
    "last_ai_interrupted = False\n",
    "vad_buffer = np.zeros(0, dtype=np.float32)\n",
    "energy_history = deque(maxlen=15)\n",
    "pause_history = deque(maxlen=5)\n",
    "micro_spike_times = deque(maxlen=5)\n",
    "\n",
    "stream.start()\n",
    "print(\"ğŸ™ï¸ Real-time conversation test started\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if not audio_buffer:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # ---- COLLECT AUDIO CHUNK ----\n",
    "        chunk = audio_buffer.pop(0).astype(np.float32).flatten()\n",
    "        vad_buffer = np.concatenate([vad_buffer, chunk])\n",
    "\n",
    "        if len(vad_buffer) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        frame = vad_buffer[:VAD_MIN_SAMPLES]\n",
    "        vad_buffer = vad_buffer[VAD_MIN_SAMPLES:]\n",
    "        if len(frame) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        rms = np.sqrt(np.mean(frame ** 2))\n",
    "        energy_history.append(rms)\n",
    "\n",
    "        # ---- VAD ----\n",
    "        with torch.no_grad():\n",
    "            vad_confidence = vad_model(torch.from_numpy(frame).unsqueeze(0), 16000).item()\n",
    "        speech_started = vad_confidence > 0.5\n",
    "        sustained = sum(e > ENERGY_FLOOR for e in energy_history) >= 3\n",
    "\n",
    "        if speech_started or sustained:\n",
    "            human_speaking_now.set()\n",
    "        else:\n",
    "            human_speaking_now.clear()\n",
    "\n",
    "        # ---- HUMAN INTERRUPTION DETECTION (edge-triggered) ----\n",
    "        if speech_started and not human_interrupt_event.is_set():\n",
    "            human_interrupt_event.set()\n",
    "\n",
    "            if ai_speaking_event.is_set() and not ai_interrupt_latched:\n",
    "                ai_interrupt_latched = True\n",
    "                sd.stop()  # ğŸ”¥ HARD STOP CURRENT PLAYBACK\n",
    "                print(\"ğŸ›‘ Human interrupted AI\")\n",
    "\n",
    "        # ---- MICRO-SPIKE DETECTION ----\n",
    "        if state == \"PAUSING\" and rms > ENERGY_FLOOR:\n",
    "            micro_spike_times.append(now)\n",
    "\n",
    "        # ---- STATE MACHINE ----\n",
    "        if not speech_started and state == \"IDLE\":\n",
    "            human_interrupt_event.clear()\n",
    "\n",
    "        if state == \"IDLE\":\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech started\")\n",
    "\n",
    "        elif state == \"SPEAKING\":\n",
    "            if speech_started or sustained:\n",
    "                last_voice_time = now\n",
    "            else:\n",
    "                elapsed = (now - last_voice_time) * 1000\n",
    "                if elapsed >= PAUSE_MS:\n",
    "                    state = \"PAUSING\"\n",
    "                    print(f\"ğŸŸ¡ Pause {int(elapsed)} ms\")\n",
    "\n",
    "        elif state == \"PAUSING\":\n",
    "            elapsed = (now - last_voice_time) * 1000\n",
    "\n",
    "            # SAFETY TIMEOUT\n",
    "            if elapsed > SAFETY_TIMEOUT_MS:\n",
    "                print(f\"ğŸ”´ SAFETY TIMEOUT: Force-ending turn after {elapsed:.0f}ms\")\n",
    "                state = \"IDLE\"\n",
    "                last_voice_time = None\n",
    "                energy_history.clear()\n",
    "                pause_history.clear()\n",
    "                micro_spike_times.clear()\n",
    "                last_ai_interrupted = False\n",
    "                with turn_audio_lock:\n",
    "                    turn_audio.clear()\n",
    "                current_partial_text = \"\"\n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    vosk_reset_requested = True\n",
    "                continue\n",
    "\n",
    "            # RESUME SPEECH?\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech resumed\")\n",
    "            else:\n",
    "                # CALCULATE CONFIDENCE\n",
    "                confidence = 0.0\n",
    "                if elapsed > END_MS:\n",
    "                    confidence += 1.0\n",
    "                if len(energy_history) >= 8:\n",
    "                    recent_energies = list(energy_history)[-8:]\n",
    "                    if max(recent_energies) < ENERGY_FLOOR * 1.8:\n",
    "                        confidence += 0.7\n",
    "                if elapsed < 1000:\n",
    "                    recent_spikes = [t for t in micro_spike_times if now - t < 0.6]\n",
    "                    if len(recent_spikes) >= 2:\n",
    "                        confidence -= 0.5\n",
    "                if elapsed < 900 and current_partial_text:\n",
    "                    confidence += lexical_bias(current_partial_text) * 0.6\n",
    "                if last_ai_interrupted:\n",
    "                    confidence -= 0.5\n",
    "\n",
    "                # END TURN?\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"ğŸ”´ Turn ended (confidence={confidence:.2f}, silence={elapsed:.0f}ms)\")\n",
    "\n",
    "                    human_interrupt_event.clear()\n",
    "\n",
    "                    # CAPTURE FULL TURN AUDIO\n",
    "                    with turn_audio_lock:\n",
    "                        turn_frames = list(turn_audio)\n",
    "                        turn_audio.clear()\n",
    "\n",
    "                    # RESET STATE\n",
    "                    state = \"IDLE\"\n",
    "                    last_voice_time = None\n",
    "                    energy_history.clear()\n",
    "                    pause_history.clear()\n",
    "                    micro_spike_times.clear()\n",
    "                    last_ai_interrupted = False\n",
    "                    current_partial_text = \"\"\n",
    "                    if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                        vosk_reset_requested = True\n",
    "\n",
    "                    # FIX: Create timing object and pass it to the thread\n",
    "                    timing = TurnTiming(turn_id=turn_counter)\n",
    "                    threading.Thread(\n",
    "                        target=generate_response, \n",
    "                        args=(turn_frames, timing),  # <-- Pass both arguments!\n",
    "                        daemon=True\n",
    "                    ).start()\n",
    "                    \n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "        # ---- BUFFER AUDIO FOR STREAMING AND FINAL TRANSCRIPTION ----\n",
    "        if state in (\"SPEAKING\", \"PAUSING\"):\n",
    "            # For final transcription (never trimmed until turn ends)\n",
    "            with turn_audio_lock:\n",
    "                turn_audio.append((frame.copy(), now))\n",
    "            # For streaming partials\n",
    "            with asr_lock:\n",
    "                asr_audio.append((frame.copy(), now))\n",
    "                if TRANSCRIPTION_MODE == \"whisper\":\n",
    "                    cutoff = now - WHISPER_WINDOW_SEC\n",
    "                    while asr_audio and asr_audio[0][1] < cutoff:\n",
    "                        asr_audio.popleft()\n",
    "            # Vosk internal buffer\n",
    "            if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                if not hasattr(asr_worker, \"vosk_buf\"):\n",
    "                    asr_worker.vosk_buf = np.zeros(0, dtype=np.float32)\n",
    "                asr_worker.vosk_buf = np.concatenate([asr_worker.vosk_buf, frame])\n",
    "                while len(asr_worker.vosk_buf) >= VOSK_MIN_SAMPLES:\n",
    "                    chunk_to_send = asr_worker.vosk_buf[:VOSK_MIN_SAMPLES]\n",
    "                    asr_worker.vosk_buf = asr_worker.vosk_buf[VOSK_MIN_SAMPLES:]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"\\nğŸ›‘ Test stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5499235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "ğŸ“Š SESSION BENCHMARK SUMMARY (15 turns)\n",
      "======================================================================\n",
      "Audio Capture       :    0.2ms avg [   0.1 -    1.6]\n",
      "Whisper Transcribe  : 1158.2ms avg [1099.5 - 1236.8]\n",
      "LLM Generation      : 1293.9ms avg [ 481.6 - 2802.7]\n",
      "Total Latency       : 2479.2ms avg [1644.0 - 4416.9]\n",
      "\n",
      "Whisper RTF: 0.32x (lower is better, <1.0 = real-time)\n",
      "======================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# %% =============================\n",
    "# BENCHMARK SUMMARY TOOL\n",
    "# =============================\n",
    "def print_benchmark_summary():\n",
    "    \"\"\"Call this manually after a session to see aggregate stats\"\"\"\n",
    "    if not timing_history:\n",
    "        print(\"No timing data recorded yet\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š SESSION BENCHMARK SUMMARY ({len(timing_history)} turns)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    stages = [\n",
    "        (\"Audio Capture\", \"audio_capture_duration_ms\"),\n",
    "        (\"Whisper Transcribe\", \"whisper_transcribe_ms\"),\n",
    "        (\"LLM Tokenization\", \"llm_tokenize_ms\"),\n",
    "        (\"LLM Generation\", \"llm_generate_ms\"),\n",
    "        (\"Text Processing\", \"text_process_ms\"),\n",
    "        (\"TTS Generation\", \"tts_generate_ms\"),\n",
    "        (\"Total Latency\", \"total_latency_ms\")\n",
    "    ]\n",
    "    \n",
    "    for name, attr in stages:\n",
    "        values = [getattr(t, attr) for t in timing_history if getattr(t, attr) > 0]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            mn, mx = min(values), max(values)\n",
    "            print(f\"{name:20s}: {avg:6.1f}ms avg [{mn:6.1f} - {mx:6.1f}]\")\n",
    "    \n",
    "    # RTF analysis\n",
    "    rtfs = [t.whisper_rtf for t in timing_history if t.whisper_rtf > 0]\n",
    "    if rtfs:\n",
    "        print(f\"\\nWhisper RTF: {sum(rtfs)/len(rtfs):.2f}x (lower is better, <1.0 = real-time)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Run this anytime to see stats:\n",
    "print_benchmark_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0c093c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
