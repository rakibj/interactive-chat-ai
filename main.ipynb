{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5f817fb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[68]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mNUMEXPR_NUM_THREADS\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     22\u001b[39m torch.set_num_threads(\u001b[32m8\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_num_interop_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m torch.set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from collections import deque\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\"  # Only show errors\n",
    "from llama_cpp import Llama\n",
    "import os\n",
    "\n",
    "# Optimize CPU threading for inference\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(1)\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    from pocket_tts import TTSModel\n",
    "    POCKET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POCKET_AVAILABLE = False\n",
    "    print(\"âš ï¸ pocket-tts not installed. Install with: uv add pocket-tts\")\n",
    "\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "TRANSCRIPTION_MODE = \"vosk\"  # Options: \"vosk\" (fast partials) or \"whisper\"\n",
    "PROJECT_ROOT = r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\"\n",
    "TTS_MODE = \"pocket\"  # Options: \"pocket\" (neural) or \"powershell\" (system)\n",
    "POCKET_VOICE = \"alba\"  # Options: alba, marius, javert, jean, fantine, cosette, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "25a1a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PC/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (for final transcription)...\n",
      "Loading Vosk...\n",
      "ASR mode: vosk\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# LOAD MODELS (ALWAYS LOAD WHISPER FOR FINAL TRANSCRIPTION)\n",
    "# =============================\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "\n",
    "print(\"Loading Whisper (for final transcription)...\")\n",
    "whisper = WhisperModel(\n",
    "    r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\whisper\\distil-small.en\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\",\n",
    "    local_files_only=True,  # Force local, no hub download\n",
    "    cpu_threads=8\n",
    ")\n",
    "\n",
    "# Load Vosk only if needed\n",
    "vosk_model = None\n",
    "vosk_rec = None\n",
    "if TRANSCRIPTION_MODE == \"vosk\":\n",
    "    from vosk import Model, KaldiRecognizer\n",
    "    print(\"Loading Vosk...\")\n",
    "    vosk_model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    vosk_rec = KaldiRecognizer(vosk_model, 16000)\n",
    "    vosk_rec.SetWords(True)\n",
    "\n",
    "print(f\"ASR mode: {TRANSCRIPTION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c6b0f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Checking model path: D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\llm\\qwen2.5-3b-instruct-q5_k_m.gguf\n",
      "   Exists? True\n",
      "   Size: 2.27 GB\n"
     ]
    }
   ],
   "source": [
    "# GLOBALS (replace old _llm_model/_llm_tokenizer)\n",
    "_llama_model = None\n",
    "GGUF_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"llm\" ,\"qwen2.5-3b-instruct-q5_k_m.gguf\")  # Adjust path as needed\n",
    "\n",
    "def get_llm():\n",
    "    global _llama_model\n",
    "    if _llama_model is None:\n",
    "        print(\"â³ Loading Qwen2.5-3B (Q5_K_M GGUF) on CPU...\")\n",
    "        try:\n",
    "            _llama_model = Llama(\n",
    "                model_path=GGUF_MODEL_PATH,\n",
    "                n_ctx=2048,\n",
    "                n_threads=8,\n",
    "                n_threads_batch=8,\n",
    "                n_batch=512,\n",
    "                n_gqa=1,  # âš ï¸ CRITICAL FOR QWEN\n",
    "                verbose=False,  # Enable loading logs\n",
    "                use_mmap=True,\n",
    "                use_mlock=False,\n",
    "                rope_freq_base=1000000.0\n",
    "            )\n",
    "            print(\"âœ… Qwen2.5-3B loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ FAILED to load GGUF model: {e}\")\n",
    "            print(f\"   Check if file exists: {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "            raise  # Force crash to see error\n",
    "    return _llama_model\n",
    "\n",
    "# Add after defining GGUF_MODEL_PATH\n",
    "print(f\"ğŸ” Checking model path: {GGUF_MODEL_PATH}\")\n",
    "print(f\"   Exists? {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "print(f\"   Size: {os.path.getsize(GGUF_MODEL_PATH) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "089cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR worker started\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# AUDIO SETUP\n",
    "# =============================\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = []\n",
    "VOSK_MIN_SAMPLES = 3200  # 0.2 sec @ 16kHz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_buffer.append(indata.copy())\n",
    "\n",
    "stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ASR WORKER (STREAMING PARTIALS)\n",
    "# =============================\n",
    "asr_audio = deque()      # For streaming partials (trimmed)\n",
    "turn_audio = deque()     # For final transcription (full turn)\n",
    "asr_lock = threading.Lock()\n",
    "turn_audio_lock = threading.Lock()\n",
    "current_partial_text = \"\"\n",
    "vosk_reset_requested = False\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def asr_worker():\n",
    "    global current_partial_text, vosk_reset_requested\n",
    "    WHISPER_WINDOW_SEC = 1.2\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05 if TRANSCRIPTION_MODE == \"vosk\" else 0.7)\n",
    "\n",
    "        if TRANSCRIPTION_MODE == \"whisper\":\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                now = time.time()\n",
    "                recent = [frame for frame, t in asr_audio if now - t <= WHISPER_WINDOW_SEC]\n",
    "            if not recent:\n",
    "                continue\n",
    "            audio_np = np.concatenate(recent)\n",
    "            segments, _ = whisper.transcribe(\n",
    "                audio_np, language=\"en\", vad_filter=False, beam_size=1, temperature=0.0\n",
    "            )\n",
    "            text = \" \".join(seg.text for seg in segments).strip()\n",
    "            if text and text != current_partial_text:\n",
    "                current_partial_text = text\n",
    "                print(\"ğŸ“ Partial:\", text)\n",
    "\n",
    "        else:  # Vosk mode\n",
    "            if vosk_reset_requested:\n",
    "                vosk_rec.Reset()\n",
    "                vosk_reset_requested = False\n",
    "                current_partial_text = \"\"\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                frame, _ = asr_audio.popleft()\n",
    "                if len(frame) < VOSK_MIN_SAMPLES:\n",
    "                    continue\n",
    "                pcm16 = float32_to_int16(frame)\n",
    "            try:\n",
    "                if vosk_rec.AcceptWaveform(pcm16.tobytes()):\n",
    "                    res = json.loads(vosk_rec.Result())\n",
    "                    text = res.get(\"text\", \"\").strip()\n",
    "                    if text:\n",
    "                        print(\"ğŸ“ Final:\", text)\n",
    "                        current_partial_text = \"\"\n",
    "                else:\n",
    "                    res = json.loads(vosk_rec.PartialResult())\n",
    "                    partial = res.get(\"partial\", \"\").strip()\n",
    "                    if partial and partial != current_partial_text:\n",
    "                        current_partial_text = partial\n",
    "                        print(\"ğŸ“ Partial:\", partial)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "threading.Thread(target=asr_worker, daemon=True).start()\n",
    "print(\"ASR worker started\")\n",
    "\n",
    "# =============================\n",
    "# TURN-TAKING RULES\n",
    "# =============================\n",
    "TRAILING_CONJUNCTIONS = {\"and\",\"or\",\"but\",\"because\",\"so\",\"that\",\"which\",\"who\",\"when\",\"if\",\"though\",\"while\"}\n",
    "OPEN_ENDED_PREFIXES = (\"i think\",\"i guess\",\"i'm not sure\",\"the thing is\",\"it depends\")\n",
    "QUESTION_LEADINS = (\"do you think\",\"would you say\",\"is it possible\",\"can you\")\n",
    "SELF_REPAIR_MARKERS = (\"i mean\",\"actually\",\"sorry\",\"no wait\")\n",
    "FILLER_ENDINGS = (\"uh\",\"um\",\"like\",\"you know\",\"kind of\")\n",
    "\n",
    "def lexical_bias(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    t = text.lower().strip()\n",
    "    words = t.split()\n",
    "    score = 0.0\n",
    "    if words[-1] in TRAILING_CONJUNCTIONS: score -= 1.0\n",
    "    if any(t.startswith(p) for p in OPEN_ENDED_PREFIXES): score -= 0.6\n",
    "    if any(t.startswith(q) for q in QUESTION_LEADINS): score -= 0.5\n",
    "    if any(m in t[-20:] for m in SELF_REPAIR_MARKERS): score -= 0.4\n",
    "    if words[-1] in FILLER_ENDINGS: score -= 0.7\n",
    "    return score\n",
    "\n",
    "def energy_decay_score(energy_history):\n",
    "    if len(energy_history) < 5: return 0.0\n",
    "    x = np.arange(len(energy_history))\n",
    "    y = np.array(energy_history)\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    return 0.8 if slope < -0.00015 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a017d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# POCKET TTS LOADING\n",
    "# =============================\n",
    "_pocket_model = None\n",
    "_pocket_voice_state = None\n",
    "_pocket_sample_rate = 24000  # Pocket TTS default\n",
    "\n",
    "def get_pocket_tts():\n",
    "    \"\"\"Lazy load Pocket TTS model\"\"\"\n",
    "    global _pocket_model, _pocket_voice_state, _pocket_sample_rate\n",
    "    if _pocket_model is None:\n",
    "        if not POCKET_AVAILABLE:\n",
    "            raise ImportError(\"pocket-tts not installed\")\n",
    "        print(f\"â³ Loading Pocket TTS (voice: {POCKET_VOICE})...\")\n",
    "        _pocket_model = TTSModel.load_model()\n",
    "        _pocket_voice_state = _pocket_model.get_state_for_audio_prompt(POCKET_VOICE)\n",
    "        _pocket_sample_rate = _pocket_model.sample_rate\n",
    "        print(\"âœ… Pocket TTS loaded!\")\n",
    "    return _pocket_model, _pocket_voice_state, _pocket_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f5c3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Speak text using configured TTS backend\"\"\"\n",
    "    if not text:\n",
    "        return\n",
    "    \n",
    "    if TTS_MODE == \"pocket\":\n",
    "        try:\n",
    "            model, voice_state, sr = get_pocket_tts()\n",
    "            \n",
    "            # Generate audio (returns torch tensor)\n",
    "            audio = model.generate_audio(voice_state, text)\n",
    "            \n",
    "            # Convert to numpy and play using sounddevice (already imported)\n",
    "            audio_np = audio.numpy() if hasattr(audio, 'numpy') else np.array(audio)\n",
    "            \n",
    "            # Normalize if needed\n",
    "            if audio_np.max() > 1.0:\n",
    "                audio_np = audio_np / 32767.0\n",
    "            \n",
    "            # Play audio (blocking is fine here since we're in a dedicated thread)\n",
    "            sd.play(audio_np, sr)\n",
    "            sd.wait()  # Wait until audio finishes playing\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ğŸ”Š Pocket TTS error: {e}\")\n",
    "            print(\"ğŸ”„ Falling back to PowerShell TTS...\")\n",
    "            speak_powershell(text)\n",
    "    else:\n",
    "        speak_powershell(text)\n",
    "\n",
    "def speak_powershell(text):\n",
    "    \"\"\"Original Windows PowerShell TTS (fallback)\"\"\"\n",
    "    safe_text = text.replace('\"', '\"\"').replace('\\n', ' ').replace('\\r', '')\n",
    "    cmd = f'Add-Type -AssemblyName System.Speech; $s=New-Object System.Speech.Synthesis.SpeechSynthesizer; $s.Speak(\"{safe_text}\")'\n",
    "    try:\n",
    "        subprocess.run([\"powershell\", \"-Command\", cmd],\n",
    "                       stdout=subprocess.DEVNULL,\n",
    "                       stderr=subprocess.DEVNULL,\n",
    "                       timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"ğŸ”Š Speech error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4d8cb50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… PowerShell TTS initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# WINDOWS-RELIABLE TTS (POWER SHELL)\n",
    "# =============================\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def tts_main_loop():\n",
    "    \"\"\"Main thread TTS loop (never fails on Windows)\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            text = response_queue.get(timeout=0.1)\n",
    "            print(f\"ğŸ—£ï¸ Speaking: '{text}'\")\n",
    "            speak(text)\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "# Start TTS loop in background (non-daemon = survives between turns)\n",
    "threading.Thread(target=tts_main_loop, daemon=False).start()\n",
    "print(\"âœ… PowerShell TTS initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6e1d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the TOP of your notebook (before main loop), make sure these exist:\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class TurnTiming:\n",
    "    turn_id: int = 0\n",
    "    speech_end_time: float = 0.0\n",
    "    audio_capture_duration_ms: float = 0.0\n",
    "    whisper_transcribe_ms: float = 0.0\n",
    "    whisper_rtf: float = 0.0\n",
    "    llm_tokenize_ms: float = 0.0\n",
    "    llm_generate_ms: float = 0.0\n",
    "    llm_tokens_per_sec: float = 0.0\n",
    "    text_process_ms: float = 0.0\n",
    "    tts_generate_ms: float = 0.0\n",
    "    tts_playback_ms: float = 0.0\n",
    "    total_latency_ms: float = 0.0\n",
    "    total_audio_duration_sec: float = 0.0\n",
    "    \n",
    "    def print_report(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"ğŸ“Š TURN #{self.turn_id} TIMING AUDIT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"ğŸ™ï¸  User audio duration:     {self.total_audio_duration_sec:.2f}s\")\n",
    "        print(f\"â±ï¸  Speech end â†’ Response:   {self.total_latency_ms:.0f}ms total\")\n",
    "        print(f\"{'â”€'*40}\")\n",
    "        print(f\"1. Audio buffer capture:     {self.audio_capture_duration_ms:.1f}ms\")\n",
    "        print(f\"2. Whisper transcription:    {self.whisper_transcribe_ms:.1f}ms (RTF: {self.whisper_rtf:.2f}x)\")\n",
    "        print(f\"3. LLM tokenization:         {self.llm_tokenize_ms:.1f}ms\")\n",
    "        print(f\"4. LLM generation:           {self.llm_generate_ms:.1f}ms ({self.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        print(f\"5. Text processing:          {self.text_process_ms:.1f}ms\")\n",
    "        if self.tts_generate_ms > 0:\n",
    "            print(f\"6. TTS generation:           {self.tts_generate_ms:.1f}ms\")\n",
    "            print(f\"7. Audio playback:           {self.tts_playback_ms:.1f}ms\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "turn_counter = 0\n",
    "timing_history: List[TurnTiming] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8931e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    " # GENERATE RESPONSE\n",
    "# %% =============================\n",
    "# MODIFIED GENERATE_RESPONSE WITH TIMING\n",
    "# =============================\n",
    "def generate_response(frames, timing: TurnTiming):\n",
    "    global turn_counter\n",
    "    timing.speech_end_time = time.perf_counter()\n",
    "    SYSTEM_PROMPT = (\n",
    "        \"You are a helpful, concise AI assistant for real-time voice conversations. \"\n",
    "        \"Keep responses under 2 sentences. Speak naturally like a human. \"\n",
    "        \"Never say 'As an AI...' or mention your limitations.\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Audio Capture\n",
    "        t0 = time.perf_counter()\n",
    "        if not frames:\n",
    "            print(\"âš ï¸ No audio captured â€” skipping response\")\n",
    "            return\n",
    "        \n",
    "        full_audio = np.concatenate([frame for frame, _ in frames])\n",
    "        timing.total_audio_duration_sec = full_audio.shape[0] / 16000.0\n",
    "        timing.audio_capture_duration_ms = (time.perf_counter() - t0) * 1000\n",
    "        print(f\"ğŸ”Š Captured {len(frames)} frames ({timing.total_audio_duration_sec:.2f}s) in {timing.audio_capture_duration_ms:.1f}ms\")\n",
    "        \n",
    "        # Stage 2: Whisper Transcription\n",
    "        t1 = time.perf_counter()\n",
    "        segments, info = whisper.transcribe(\n",
    "            full_audio,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            condition_on_previous_text=False\n",
    "        )\n",
    "        user_text = \" \".join(seg.text for seg in segments).strip()\n",
    "        timing.whisper_transcribe_ms = (time.perf_counter() - t1) * 1000\n",
    "        timing.whisper_rtf = timing.whisper_transcribe_ms / (timing.total_audio_duration_sec * 1000)\n",
    "        \n",
    "        if not user_text:\n",
    "            print(\"âš ï¸ Empty transcription â€” skipping response\")\n",
    "            return\n",
    "        print(f\"ğŸ’¬ User: '{user_text}' (Whisper: {timing.whisper_transcribe_ms:.1f}ms, RTF: {timing.whisper_rtf:.2f}x)\")\n",
    "        \n",
    "        # Stage 3: LLM Tokenization\n",
    "        t2 = time.perf_counter()\n",
    "        llm_model = get_llm()\n",
    "    \n",
    "        \n",
    "        # Stage 4: LLM Generation\n",
    "        t3 = time.perf_counter()\n",
    "\n",
    "        # Use proper chat template for Qwen2.5\n",
    "        try:\n",
    "            # Use llama.cpp's built-in chat interface\n",
    "            response = llm_model.create_chat_completion(\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": user_text}\n",
    "                ],\n",
    "                max_tokens=50,\n",
    "                temperature=0.0,\n",
    "                stream=False\n",
    "            )\n",
    "            response_text = response[\"choices\"][0][\"message\"][\"content\"].strip()\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ LLM generation error: {e}\")\n",
    "            response_text = \"I'm sorry, I couldn't process that.\"\n",
    "\n",
    "        gen_time = time.perf_counter() - t3\n",
    "        output_tokens = len(response_text.split())  # Approximate\n",
    "        timing.llm_generate_ms = gen_time * 1000\n",
    "        timing.llm_tokens_per_sec = output_tokens / gen_time if gen_time > 0 else 0\n",
    "\n",
    "        print(f\"ğŸ¤– LLM: {timing.llm_generate_ms:.1f}ms ({timing.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        \n",
    "        # Stage 5: Text Processing\n",
    "        # Stage 5: Text Processing\n",
    "        t4 = time.perf_counter()\n",
    "        if TTS_MODE == \"pocket\":\n",
    "            if not response_text.strip().endswith(('.', '!', '?')):\n",
    "                response_text = response_text.strip() + '.'\n",
    "            timing.text_process_ms = (time.perf_counter() - t4) * 1000\n",
    "        else:\n",
    "            # For PowerShell TTS (more restrictive)\n",
    "            response_text = response_text.encode('ascii', 'ignore').decode('ascii')\n",
    "            if not response_text.strip().endswith(('.', '!', '?')):\n",
    "                response_text = response_text.strip() + '.'\n",
    "\n",
    "        timing.text_process_ms = (time.perf_counter() - t4) * 1000\n",
    "        print(f\"ğŸ—£ï¸  Final TTS text: '{response_text}' (processing: {timing.text_process_ms:.1f}ms)\")\n",
    "        \n",
    "        # Stage 6 & 7: TTS Generation + Playback\n",
    "        if TTS_MODE == \"pocket\":\n",
    "            t5 = time.perf_counter()\n",
    "            try:\n",
    "                model, voice_state, sr = get_pocket_tts()\n",
    "                audio = model.generate_audio(voice_state, response_text)\n",
    "                timing.tts_generate_ms = (time.perf_counter() - t5) * 1000\n",
    "                \n",
    "                audio_np = audio.numpy() if hasattr(audio, 'numpy') else np.array(audio)\n",
    "                if audio_np.max() > 1.0:\n",
    "                    audio_np = audio_np / 32767.0\n",
    "                \n",
    "                t6 = time.perf_counter()\n",
    "                sd.play(audio_np, sr)\n",
    "                sd.wait()\n",
    "                timing.tts_playback_ms = (time.perf_counter() - t6) * 1000\n",
    "                print(f\"ğŸ”Š TTS: {timing.tts_generate_ms:.1f}ms gen + {timing.tts_playback_ms:.1f}ms playback\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"ğŸ”Š Pocket TTS error: {e}, falling back...\")\n",
    "                t5 = time.perf_counter()\n",
    "                speak_powershell(response_text)\n",
    "                timing.tts_generate_ms = (time.perf_counter() - t5) * 1000\n",
    "                timing.tts_playback_ms = 0  # Unknown for powershell\n",
    "        else:\n",
    "            t5 = time.perf_counter()\n",
    "            speak_powershell(response_text)\n",
    "            timing.tts_generate_ms = (time.perf_counter() - t5) * 1000\n",
    "            timing.tts_playback_ms = 0\n",
    "        \n",
    "        # Final Report\n",
    "        timing.total_latency_ms = (time.perf_counter() - timing.speech_end_time) * 1000\n",
    "        timing.print_report()\n",
    "        timing_history.append(timing)\n",
    "        turn_counter += 1\n",
    "        \n",
    "        # Running statistics\n",
    "        if len(timing_history) > 1:\n",
    "            avg_latency = sum(t.total_latency_ms for t in timing_history) / len(timing_history)\n",
    "            print(f\"ğŸ“ˆ Running average latency: {avg_latency:.0f}ms over {len(timing_history)} turns\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "20b88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ™ï¸ Real-time conversation test started\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 631 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1252ms)\n",
      "ğŸ”Š Captured 83 frames (2.66s) in 0.1ms\n",
      "ğŸ’¬ User: 'Hey, how's it going?' (Whisper: 996.8ms, RTF: 0.38x)\n",
      "ğŸ¤– LLM: 1635.9ms (4.9 tok/s)\n",
      "ğŸ—£ï¸  Final TTS text: 'Hey there! Going well, thanks. How about you?' (processing: 0.0ms)\n",
      "ğŸ”Š TTS: 645.1ms gen + 3449.8ms playback\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #23 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     2.66s\n",
      "â±ï¸  Speech end â†’ Response:   6730ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    996.8ms (RTF: 0.38x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1635.9ms (4.9 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "6. TTS generation:           645.1ms\n",
      "7. Audio playback:           3449.8ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 9294ms over 11 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 631 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1229ms)\n",
      "ğŸ”Š Captured 101 frames (3.23s) in 0.1ms\n",
      "ğŸ’¬ User: 'doing good too? What are you doing?' (Whisper: 1031.9ms, RTF: 0.32x)\n",
      "ğŸ¤– LLM: 1062.8ms (11.3 tok/s)\n",
      "ğŸ—£ï¸  Final TTS text: 'Doing well, thanks! Just catching up on some reading. How about you?' (processing: 0.0ms)\n",
      "ğŸ”Š TTS: 1500.3ms gen + 4960.3ms playback\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #25 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     3.23s\n",
      "â±ï¸  Speech end â†’ Response:   8557ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1031.9ms (RTF: 0.32x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1062.8ms (11.3 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "6. TTS generation:           1500.3ms\n",
      "7. Audio playback:           4960.3ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 9233ms over 12 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 600 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1251ms)\n",
      "ğŸ”Š Captured 206 frames (6.59s) in 0.1ms\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸ’¬ User: 'also doing well what can we do I mean as an element LLLM assistant' (Whisper: 1132.2ms, RTF: 0.17x)\n",
      "ğŸŸ¡ Pause 616 ms\n",
      "ğŸ¤– LLM: 1155.6ms (9.5 tok/s)\n",
      "ğŸ—£ï¸  Final TTS text: 'Let's brainstorm projects or challenges to enhance our skills and knowledge.' (processing: 0.0ms)\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1217ms)\n",
      "ğŸ”Š Captured 47 frames (1.50s) in 0.1ms\n",
      "ğŸ’¬ User: 'Thanks.' (Whisper: 1650.3ms, RTF: 1.10x)\n",
      "ğŸ¤– LLM: 837.7ms (9.6 tok/s)\n",
      "ğŸ—£ï¸  Final TTS text: 'You're welcome. How can I help you today?' (processing: 0.0ms)\n",
      "ğŸ”Š TTS: 1487.8ms gen + 2358.9ms playback\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #27 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     6.59s\n",
      "â±ï¸  Speech end â†’ Response:   6137ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1132.2ms (RTF: 0.17x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1155.6ms (9.5 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "6. TTS generation:           1487.8ms\n",
      "7. Audio playback:           2358.9ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 8995ms over 13 turns\n",
      "ğŸ”Š TTS: 1035.0ms gen + 2765.9ms playback\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #28 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     1.50s\n",
      "â±ï¸  Speech end â†’ Response:   6291ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1650.3ms (RTF: 1.10x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           837.7ms (9.6 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "6. TTS generation:           1035.0ms\n",
      "7. Audio playback:           2765.9ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 8802ms over 14 turns\n",
      "ğŸŸ¢ Speech started\n",
      "ğŸŸ¡ Pause 621 ms\n",
      "ğŸ”´ Turn ended (confidence=1.70, silence=1220ms)\n",
      "ğŸ”Š Captured 94 frames (3.01s) in 0.1ms\n",
      "ğŸ’¬ User: 'What can it do as an AI assistant?' (Whisper: 1121.3ms, RTF: 0.37x)\n",
      "ğŸ¤– LLM: 1030.9ms (9.7 tok/s)\n",
      "ğŸ—£ï¸  Final TTS text: 'I can provide information, schedule reminders, and perform web searches.' (processing: 0.0ms)\n",
      "ğŸ”Š TTS: 2183.7ms gen + 4880.6ms playback\n",
      "\n",
      "============================================================\n",
      "ğŸ“Š TURN #31 TIMING AUDIT\n",
      "============================================================\n",
      "ğŸ™ï¸  User audio duration:     3.01s\n",
      "â±ï¸  Speech end â†’ Response:   9219ms total\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    1121.3ms (RTF: 0.37x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1030.9ms (9.7 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "6. TTS generation:           2183.7ms\n",
      "7. Audio playback:           4880.6ms\n",
      "============================================================\n",
      "\n",
      "ğŸ“ˆ Running average latency: 8830ms over 15 turns\n",
      "\n",
      "ğŸ›‘ Test stopped\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MAIN LOOP\n",
    "# =============================\n",
    "import tempfile\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "VAD_MIN_SAMPLES = 512\n",
    "PAUSE_MS = 600\n",
    "END_MS = 1200\n",
    "SAFETY_TIMEOUT_MS = 2500\n",
    "ENERGY_FLOOR = 0.015\n",
    "WHISPER_WINDOW_SEC = 3.0\n",
    "CONFIDENCE_THRESHOLD = 1.2\n",
    "\n",
    "# STATE\n",
    "state = \"IDLE\"\n",
    "last_voice_time = None\n",
    "last_ai_interrupted = False\n",
    "vad_buffer = np.zeros(0, dtype=np.float32)\n",
    "energy_history = deque(maxlen=15)\n",
    "pause_history = deque(maxlen=5)\n",
    "micro_spike_times = deque(maxlen=5)\n",
    "\n",
    "stream.start()\n",
    "print(\"ğŸ™ï¸ Real-time conversation test started\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if not audio_buffer:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # ---- COLLECT AUDIO CHUNK ----\n",
    "        chunk = audio_buffer.pop(0).astype(np.float32).flatten()\n",
    "        vad_buffer = np.concatenate([vad_buffer, chunk])\n",
    "\n",
    "        if len(vad_buffer) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        frame = vad_buffer[:VAD_MIN_SAMPLES]\n",
    "        vad_buffer = vad_buffer[VAD_MIN_SAMPLES:]\n",
    "        if len(frame) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        rms = np.sqrt(np.mean(frame ** 2))\n",
    "        energy_history.append(rms)\n",
    "\n",
    "        # ---- VAD ----\n",
    "        with torch.no_grad():\n",
    "            vad_confidence = vad_model(torch.from_numpy(frame).unsqueeze(0), 16000).item()\n",
    "        speech_started = vad_confidence > 0.5\n",
    "        sustained = sum(e > ENERGY_FLOOR for e in energy_history) >= 3\n",
    "\n",
    "        # ---- MICRO-SPIKE DETECTION ----\n",
    "        if state == \"PAUSING\" and rms > ENERGY_FLOOR:\n",
    "            micro_spike_times.append(now)\n",
    "\n",
    "        # ---- STATE MACHINE ----\n",
    "        if state == \"IDLE\":\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech started\")\n",
    "\n",
    "        elif state == \"SPEAKING\":\n",
    "            if speech_started or sustained:\n",
    "                last_voice_time = now\n",
    "            else:\n",
    "                elapsed = (now - last_voice_time) * 1000\n",
    "                if elapsed >= PAUSE_MS:\n",
    "                    state = \"PAUSING\"\n",
    "                    print(f\"ğŸŸ¡ Pause {int(elapsed)} ms\")\n",
    "\n",
    "        elif state == \"PAUSING\":\n",
    "            elapsed = (now - last_voice_time) * 1000\n",
    "\n",
    "            # SAFETY TIMEOUT\n",
    "            if elapsed > SAFETY_TIMEOUT_MS:\n",
    "                print(f\"ğŸ”´ SAFETY TIMEOUT: Force-ending turn after {elapsed:.0f}ms\")\n",
    "                state = \"IDLE\"\n",
    "                last_voice_time = None\n",
    "                energy_history.clear()\n",
    "                pause_history.clear()\n",
    "                micro_spike_times.clear()\n",
    "                last_ai_interrupted = False\n",
    "                with turn_audio_lock:\n",
    "                    turn_audio.clear()\n",
    "                current_partial_text = \"\"\n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    vosk_reset_requested = True\n",
    "                continue\n",
    "\n",
    "            # RESUME SPEECH?\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"ğŸŸ¢ Speech resumed\")\n",
    "            else:\n",
    "                # CALCULATE CONFIDENCE\n",
    "                confidence = 0.0\n",
    "                if elapsed > END_MS:\n",
    "                    confidence += 1.0\n",
    "                if len(energy_history) >= 8:\n",
    "                    recent_energies = list(energy_history)[-8:]\n",
    "                    if max(recent_energies) < ENERGY_FLOOR * 1.8:\n",
    "                        confidence += 0.7\n",
    "                if elapsed < 1000:\n",
    "                    recent_spikes = [t for t in micro_spike_times if now - t < 0.6]\n",
    "                    if len(recent_spikes) >= 2:\n",
    "                        confidence -= 0.5\n",
    "                if elapsed < 900 and current_partial_text:\n",
    "                    confidence += lexical_bias(current_partial_text) * 0.6\n",
    "                if last_ai_interrupted:\n",
    "                    confidence -= 0.5\n",
    "\n",
    "                # END TURN?\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"ğŸ”´ Turn ended (confidence={confidence:.2f}, silence={elapsed:.0f}ms)\")\n",
    "\n",
    "                    # CAPTURE FULL TURN AUDIO\n",
    "                    with turn_audio_lock:\n",
    "                        turn_frames = list(turn_audio)\n",
    "                        turn_audio.clear()\n",
    "\n",
    "                    # RESET STATE\n",
    "                    state = \"IDLE\"\n",
    "                    last_voice_time = None\n",
    "                    energy_history.clear()\n",
    "                    pause_history.clear()\n",
    "                    micro_spike_times.clear()\n",
    "                    last_ai_interrupted = False\n",
    "                    current_partial_text = \"\"\n",
    "                    if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                        vosk_reset_requested = True\n",
    "\n",
    "                    # FIX: Create timing object and pass it to the thread\n",
    "                    timing = TurnTiming(turn_id=turn_counter)\n",
    "                    threading.Thread(\n",
    "                        target=generate_response, \n",
    "                        args=(turn_frames, timing),  # <-- Pass both arguments!\n",
    "                        daemon=True\n",
    "                    ).start()\n",
    "                    \n",
    "                    turn_counter += 1  # Increment for next turn\n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "        # ---- BUFFER AUDIO FOR STREAMING AND FINAL TRANSCRIPTION ----\n",
    "        if state in (\"SPEAKING\", \"PAUSING\"):\n",
    "            # For final transcription (never trimmed until turn ends)\n",
    "            with turn_audio_lock:\n",
    "                turn_audio.append((frame.copy(), now))\n",
    "            # For streaming partials\n",
    "            with asr_lock:\n",
    "                asr_audio.append((frame.copy(), now))\n",
    "                if TRANSCRIPTION_MODE == \"whisper\":\n",
    "                    cutoff = now - WHISPER_WINDOW_SEC\n",
    "                    while asr_audio and asr_audio[0][1] < cutoff:\n",
    "                        asr_audio.popleft()\n",
    "            # Vosk internal buffer\n",
    "            if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                if not hasattr(asr_worker, \"vosk_buf\"):\n",
    "                    asr_worker.vosk_buf = np.zeros(0, dtype=np.float32)\n",
    "                asr_worker.vosk_buf = np.concatenate([asr_worker.vosk_buf, frame])\n",
    "                while len(asr_worker.vosk_buf) >= VOSK_MIN_SAMPLES:\n",
    "                    chunk_to_send = asr_worker.vosk_buf[:VOSK_MIN_SAMPLES]\n",
    "                    asr_worker.vosk_buf = asr_worker.vosk_buf[VOSK_MIN_SAMPLES:]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"\\nğŸ›‘ Test stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% =============================\n",
    "# BENCHMARK SUMMARY TOOL\n",
    "# =============================\n",
    "def print_benchmark_summary():\n",
    "    \"\"\"Call this manually after a session to see aggregate stats\"\"\"\n",
    "    if not timing_history:\n",
    "        print(\"No timing data recorded yet\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"ğŸ“Š SESSION BENCHMARK SUMMARY ({len(timing_history)} turns)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    stages = [\n",
    "        (\"Audio Capture\", \"audio_capture_duration_ms\"),\n",
    "        (\"Whisper Transcribe\", \"whisper_transcribe_ms\"),\n",
    "        (\"LLM Tokenization\", \"llm_tokenize_ms\"),\n",
    "        (\"LLM Generation\", \"llm_generate_ms\"),\n",
    "        (\"Text Processing\", \"text_process_ms\"),\n",
    "        (\"TTS Generation\", \"tts_generate_ms\"),\n",
    "        (\"Total Latency\", \"total_latency_ms\")\n",
    "    ]\n",
    "    \n",
    "    for name, attr in stages:\n",
    "        values = [getattr(t, attr) for t in timing_history if getattr(t, attr) > 0]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            mn, mx = min(values), max(values)\n",
    "            print(f\"{name:20s}: {avg:6.1f}ms avg [{mn:6.1f} - {mx:6.1f}]\")\n",
    "    \n",
    "    # RTF analysis\n",
    "    rtfs = [t.whisper_rtf for t in timing_history if t.whisper_rtf > 0]\n",
    "    if rtfs:\n",
    "        print(f\"\\nWhisper RTF: {sum(rtfs)/len(rtfs):.2f}x (lower is better, <1.0 = real-time)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Run this anytime to see stats:\n",
    "print_benchmark_summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
