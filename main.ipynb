{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f817fb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Work\\Projects\\AI\\interactive-chat-ai\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from collections import deque\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "TRANSCRIPTION_MODE = \"vosk\"  # Options: \"vosk\" (fast partials) or \"whisper\"\n",
    "PROJECT_ROOT = r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25a1a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PC/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (for final transcription)...\n",
      "Loading Vosk...\n",
      "ASR mode: vosk\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# LOAD MODELS (ALWAYS LOAD WHISPER FOR FINAL TRANSCRIPTION)\n",
    "# =============================\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "\n",
    "print(\"Loading Whisper (for final transcription)...\")\n",
    "whisper = WhisperModel(\n",
    "    \"small.en\",  # English-only = faster + more accurate\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    compute_type=\"int8\"\n",
    ")\n",
    "\n",
    "# Load Vosk only if needed\n",
    "vosk_model = None\n",
    "vosk_rec = None\n",
    "if TRANSCRIPTION_MODE == \"vosk\":\n",
    "    from vosk import Model, KaldiRecognizer\n",
    "    print(\"Loading Vosk...\")\n",
    "    vosk_model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    vosk_rec = KaldiRecognizer(vosk_model, 16000)\n",
    "    vosk_rec.SetWords(True)\n",
    "\n",
    "print(f\"ASR mode: {TRANSCRIPTION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b0f242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# LLM + TTS LOADING (SIMPLIFIED FOR WINDOWS)\n",
    "# =============================\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import piper\n",
    "\n",
    "# Use TinyLlama instead of Phi-3\n",
    "LLM_PATH = os.path.join(PROJECT_ROOT, \"models\", \"llm\", \"tinyllama\")\n",
    "\n",
    "_llm_model = None\n",
    "_llm_tokenizer = None\n",
    "\n",
    "def get_llm():\n",
    "    global _llm_model, _llm_tokenizer\n",
    "    if _llm_model is None:\n",
    "        print(\"‚è≥ Loading TinyLlama (1.1B)...\")\n",
    "        _llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "            LLM_PATH,\n",
    "            device_map=\"cpu\",  # CPU-only for stability\n",
    "            torch_dtype=torch.float32  # Avoid float16 on CPU\n",
    "        )\n",
    "        _llm_tokenizer = AutoTokenizer.from_pretrained(LLM_PATH)\n",
    "        print(\"‚úÖ LLM loaded!\")\n",
    "    return _llm_model, _llm_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "089cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR worker started\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# AUDIO SETUP\n",
    "# =============================\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = []\n",
    "VOSK_MIN_SAMPLES = 3200  # 0.2 sec @ 16kHz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_buffer.append(indata.copy())\n",
    "\n",
    "stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ASR WORKER (STREAMING PARTIALS)\n",
    "# =============================\n",
    "asr_audio = deque()      # For streaming partials (trimmed)\n",
    "turn_audio = deque()     # For final transcription (full turn)\n",
    "asr_lock = threading.Lock()\n",
    "turn_audio_lock = threading.Lock()\n",
    "current_partial_text = \"\"\n",
    "vosk_reset_requested = False\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def asr_worker():\n",
    "    global current_partial_text, vosk_reset_requested\n",
    "    WHISPER_WINDOW_SEC = 1.2\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05 if TRANSCRIPTION_MODE == \"vosk\" else 0.7)\n",
    "\n",
    "        if TRANSCRIPTION_MODE == \"whisper\":\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                now = time.time()\n",
    "                recent = [frame for frame, t in asr_audio if now - t <= WHISPER_WINDOW_SEC]\n",
    "            if not recent:\n",
    "                continue\n",
    "            audio_np = np.concatenate(recent)\n",
    "            segments, _ = whisper.transcribe(\n",
    "                audio_np, language=\"en\", vad_filter=False, beam_size=1, temperature=0.0\n",
    "            )\n",
    "            text = \" \".join(seg.text for seg in segments).strip()\n",
    "            if text and text != current_partial_text:\n",
    "                current_partial_text = text\n",
    "                print(\"üìù Partial:\", text)\n",
    "\n",
    "        else:  # Vosk mode\n",
    "            if vosk_reset_requested:\n",
    "                vosk_rec.Reset()\n",
    "                vosk_reset_requested = False\n",
    "                current_partial_text = \"\"\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                frame, _ = asr_audio.popleft()\n",
    "                if len(frame) < VOSK_MIN_SAMPLES:\n",
    "                    continue\n",
    "                pcm16 = float32_to_int16(frame)\n",
    "            try:\n",
    "                if vosk_rec.AcceptWaveform(pcm16.tobytes()):\n",
    "                    res = json.loads(vosk_rec.Result())\n",
    "                    text = res.get(\"text\", \"\").strip()\n",
    "                    if text:\n",
    "                        print(\"üìù Final:\", text)\n",
    "                        current_partial_text = \"\"\n",
    "                else:\n",
    "                    res = json.loads(vosk_rec.PartialResult())\n",
    "                    partial = res.get(\"partial\", \"\").strip()\n",
    "                    if partial and partial != current_partial_text:\n",
    "                        current_partial_text = partial\n",
    "                        print(\"üìù Partial:\", partial)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "threading.Thread(target=asr_worker, daemon=True).start()\n",
    "print(\"ASR worker started\")\n",
    "\n",
    "# =============================\n",
    "# TURN-TAKING RULES\n",
    "# =============================\n",
    "TRAILING_CONJUNCTIONS = {\"and\",\"or\",\"but\",\"because\",\"so\",\"that\",\"which\",\"who\",\"when\",\"if\",\"though\",\"while\"}\n",
    "OPEN_ENDED_PREFIXES = (\"i think\",\"i guess\",\"i'm not sure\",\"the thing is\",\"it depends\")\n",
    "QUESTION_LEADINS = (\"do you think\",\"would you say\",\"is it possible\",\"can you\")\n",
    "SELF_REPAIR_MARKERS = (\"i mean\",\"actually\",\"sorry\",\"no wait\")\n",
    "FILLER_ENDINGS = (\"uh\",\"um\",\"like\",\"you know\",\"kind of\")\n",
    "\n",
    "def lexical_bias(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    t = text.lower().strip()\n",
    "    words = t.split()\n",
    "    score = 0.0\n",
    "    if words[-1] in TRAILING_CONJUNCTIONS: score -= 1.0\n",
    "    if any(t.startswith(p) for p in OPEN_ENDED_PREFIXES): score -= 0.6\n",
    "    if any(t.startswith(q) for q in QUESTION_LEADINS): score -= 0.5\n",
    "    if any(m in t[-20:] for m in SELF_REPAIR_MARKERS): score -= 0.4\n",
    "    if words[-1] in FILLER_ENDINGS: score -= 0.7\n",
    "    return score\n",
    "\n",
    "def energy_decay_score(energy_history):\n",
    "    if len(energy_history) < 5: return 0.0\n",
    "    x = np.arange(len(energy_history))\n",
    "    y = np.array(energy_history)\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    return 0.8 if slope < -0.00015 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e699cc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PowerShell TTS initialized\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# WINDOWS-RELIABLE TTS (POWER SHELL)\n",
    "# =============================\n",
    "import subprocess\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Speak text using Windows PowerShell (100% reliable on Win 10/11)\"\"\"\n",
    "    safe_text = text.replace('\"', '\"\"').replace('\\n', ' ').replace('\\r', '')\n",
    "    cmd = f'Add-Type -AssemblyName System.Speech; $s=New-Object System.Speech.Synthesis.SpeechSynthesizer; $s.Speak(\"{safe_text}\")'\n",
    "    try:\n",
    "        subprocess.run([\"powershell\", \"-Command\", cmd],\n",
    "                       stdout=subprocess.DEVNULL,\n",
    "                       stderr=subprocess.DEVNULL,\n",
    "                       timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"üîä Speech error: {e}\")\n",
    "\n",
    "def tts_main_loop():\n",
    "    \"\"\"Main thread TTS loop (never fails on Windows)\"\"\"\n",
    "    while True:\n",
    "        try:\n",
    "            text = response_queue.get(timeout=0.1)\n",
    "            print(f\"üó£Ô∏è Speaking: '{text}'\")\n",
    "            speak(text)\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "# Start TTS loop in background (non-daemon = survives between turns)\n",
    "threading.Thread(target=tts_main_loop, daemon=False).start()\n",
    "print(\"‚úÖ PowerShell TTS initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20b88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Real-time conversation test started\n",
      "üü¢ Speech started\n",
      "üü° Pause 602 ms\n",
      "üî¥ Turn ended (confidence=1.70, silence=1203ms)\n",
      "üîä Captured 51 frames (1.63s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üí¨ User: Thank you.\n",
      "‚è≥ Loading TinyLlama (1.1B)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 201/201 [00:00<00:00, 437.02it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LLM loaded!\n",
      "üü¢ Speech started\n",
      "üü° Pause 620 ms\n",
      "ü§ñ AI: I m glad I could help.\n",
      "üó£Ô∏è Speaking: I m glad I could help.\n",
      "üî¥ Turn ended (confidence=1.70, silence=1219ms)\n",
      "üîä Captured 79 frames (2.53s)\n",
      "üí¨ User: Tell me something.\n",
      "ü§ñ AI: Sure here s something I love.\n",
      "üó£Ô∏è Speaking: 'Sure here s something I love.'\n",
      "üü¢ Speech started\n",
      "üü° Pause 623 ms\n",
      "üî¥ Turn ended (confidence=1.70, silence=1225ms)\n",
      "üîä Captured 103 frames (3.30s)\n",
      "üü¢ Speech started\n",
      "üí¨ User: Why are you so robotic?\n",
      "üü° Pause 622 ms\n",
      "üî¥ Turn ended (confidence=1.70, silence=1214ms)\n",
      "üîä Captured 90 frames (2.88s)\n",
      "ü§ñ AI: I m sorry for the confusion.\n",
      "üó£Ô∏è Speaking: I m sorry for the confusion.\n",
      "‚ö†Ô∏è Empty transcription ‚Äî skipping response\n",
      "\n",
      "üõë Test stopped\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MAIN LOOP\n",
    "# =============================\n",
    "import tempfile\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "VAD_MIN_SAMPLES = 512\n",
    "PAUSE_MS = 600\n",
    "END_MS = 1200\n",
    "SAFETY_TIMEOUT_MS = 2500\n",
    "ENERGY_FLOOR = 0.015\n",
    "WHISPER_WINDOW_SEC = 3.0\n",
    "CONFIDENCE_THRESHOLD = 1.2\n",
    "\n",
    "# STATE\n",
    "state = \"IDLE\"\n",
    "last_voice_time = None\n",
    "last_ai_interrupted = False\n",
    "vad_buffer = np.zeros(0, dtype=np.float32)\n",
    "energy_history = deque(maxlen=15)\n",
    "pause_history = deque(maxlen=5)\n",
    "micro_spike_times = deque(maxlen=5)\n",
    "\n",
    "stream.start()\n",
    "print(\"üéôÔ∏è Real-time conversation test started\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if not audio_buffer:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # ---- COLLECT AUDIO CHUNK ----\n",
    "        chunk = audio_buffer.pop(0).astype(np.float32).flatten()\n",
    "        vad_buffer = np.concatenate([vad_buffer, chunk])\n",
    "\n",
    "        if len(vad_buffer) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        frame = vad_buffer[:VAD_MIN_SAMPLES]\n",
    "        vad_buffer = vad_buffer[VAD_MIN_SAMPLES:]\n",
    "        if len(frame) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        rms = np.sqrt(np.mean(frame ** 2))\n",
    "        energy_history.append(rms)\n",
    "\n",
    "        # ---- VAD ----\n",
    "        with torch.no_grad():\n",
    "            vad_confidence = vad_model(torch.from_numpy(frame).unsqueeze(0), 16000).item()\n",
    "        speech_started = vad_confidence > 0.5\n",
    "        sustained = sum(e > ENERGY_FLOOR for e in energy_history) >= 3\n",
    "\n",
    "        # ---- MICRO-SPIKE DETECTION ----\n",
    "        if state == \"PAUSING\" and rms > ENERGY_FLOOR:\n",
    "            micro_spike_times.append(now)\n",
    "\n",
    "        # ---- STATE MACHINE ----\n",
    "        if state == \"IDLE\":\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"üü¢ Speech started\")\n",
    "\n",
    "        elif state == \"SPEAKING\":\n",
    "            if speech_started or sustained:\n",
    "                last_voice_time = now\n",
    "            else:\n",
    "                elapsed = (now - last_voice_time) * 1000\n",
    "                if elapsed >= PAUSE_MS:\n",
    "                    state = \"PAUSING\"\n",
    "                    print(f\"üü° Pause {int(elapsed)} ms\")\n",
    "\n",
    "        elif state == \"PAUSING\":\n",
    "            elapsed = (now - last_voice_time) * 1000\n",
    "\n",
    "            # SAFETY TIMEOUT\n",
    "            if elapsed > SAFETY_TIMEOUT_MS:\n",
    "                print(f\"üî¥ SAFETY TIMEOUT: Force-ending turn after {elapsed:.0f}ms\")\n",
    "                state = \"IDLE\"\n",
    "                last_voice_time = None\n",
    "                energy_history.clear()\n",
    "                pause_history.clear()\n",
    "                micro_spike_times.clear()\n",
    "                last_ai_interrupted = False\n",
    "                with turn_audio_lock:\n",
    "                    turn_audio.clear()\n",
    "                current_partial_text = \"\"\n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    vosk_reset_requested = True\n",
    "                continue\n",
    "\n",
    "            # RESUME SPEECH?\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"üü¢ Speech resumed\")\n",
    "            else:\n",
    "                # CALCULATE CONFIDENCE\n",
    "                confidence = 0.0\n",
    "                if elapsed > END_MS:\n",
    "                    confidence += 1.0\n",
    "                if len(energy_history) >= 8:\n",
    "                    recent_energies = list(energy_history)[-8:]\n",
    "                    if max(recent_energies) < ENERGY_FLOOR * 1.8:\n",
    "                        confidence += 0.7\n",
    "                if elapsed < 1000:\n",
    "                    recent_spikes = [t for t in micro_spike_times if now - t < 0.6]\n",
    "                    if len(recent_spikes) >= 2:\n",
    "                        confidence -= 0.5\n",
    "                if elapsed < 900 and current_partial_text:\n",
    "                    confidence += lexical_bias(current_partial_text) * 0.6\n",
    "                if last_ai_interrupted:\n",
    "                    confidence -= 0.5\n",
    "\n",
    "                # END TURN?\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"üî¥ Turn ended (confidence={confidence:.2f}, silence={elapsed:.0f}ms)\")\n",
    "\n",
    "                    # CAPTURE FULL TURN AUDIO\n",
    "                    with turn_audio_lock:\n",
    "                        turn_frames = list(turn_audio)\n",
    "                        turn_audio.clear()\n",
    "\n",
    "                    # RESET STATE\n",
    "                    state = \"IDLE\"\n",
    "                    last_voice_time = None\n",
    "                    energy_history.clear()\n",
    "                    pause_history.clear()\n",
    "                    micro_spike_times.clear()\n",
    "                    last_ai_interrupted = False\n",
    "                    current_partial_text = \"\"\n",
    "                    if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                        vosk_reset_requested = True\n",
    "\n",
    "                    # GENERATE RESPONSE\n",
    "                    def generate_response(frames):\n",
    "                        start_time = time.time()\n",
    "                        \n",
    "                        if not frames:\n",
    "                            print(\"‚ö†Ô∏è No audio captured ‚Äî skipping response\")\n",
    "                            return\n",
    "                        full_audio = np.concatenate([frame for frame, _ in frames])\n",
    "                        print(f\"üîä Captured {len(frames)} frames ({full_audio.shape[0]/16000:.2f}s)\")\n",
    "\n",
    "                        # FINAL TRANSCRIPTION WITH WHISPER\n",
    "                        segments, _ = whisper.transcribe(\n",
    "                            full_audio,\n",
    "                            language=\"en\",\n",
    "                            beam_size=5,\n",
    "                            temperature=0.0,\n",
    "                            condition_on_previous_text=False\n",
    "                        )\n",
    "                        user_text = \" \".join(seg.text for seg in segments).strip()\n",
    "                        if not user_text:\n",
    "                            print(\"‚ö†Ô∏è Empty transcription ‚Äî skipping response\")\n",
    "                            return\n",
    "                        print(f\"üí¨ User: {user_text}\")\n",
    "\n",
    "                        try:\n",
    "                            # LLM\n",
    "                            llm_model, tokenizer = get_llm()\n",
    "                            prompt = f\"<|user|>\\n{user_text}<|end|>\\n<|assistant|>\\n\"\n",
    "                            inputs = tokenizer(prompt, return_tensors=\"pt\").to(llm_model.device)\n",
    "                            with torch.no_grad():\n",
    "                                outputs = llm_model.generate(\n",
    "                                    **inputs,\n",
    "                                    max_new_tokens=20,\n",
    "                                    do_sample=False,\n",
    "                                    pad_token_id=tokenizer.eos_token_id\n",
    "                                )\n",
    "                            response_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                            response_text = response_text.split(\"<|assistant|>\")[-1].strip()\n",
    "\n",
    "                            # üî• ULTRA-AGGRESSIVE SANITIZATION (ASCII ONLY)\n",
    "                            # Remove EVERYTHING except letters, spaces, and basic punctuation\n",
    "                            response_text = re.sub(r'[^a-zA-Z\\s.!?]', ' ', response_text)\n",
    "                            response_text = re.sub(r'\\s+', ' ', response_text).strip()\n",
    "                            \n",
    "                            # Keep ONLY first 6 words MAX (Piper struggles beyond this on CPU)\n",
    "                            words = response_text.split()[:6]\n",
    "                            if not words:\n",
    "                                safe_text = \"Okay\"\n",
    "                            else:\n",
    "                                safe_text = ' '.join(words)\n",
    "                                # Ensure ends with punctuation\n",
    "                                if not safe_text.endswith(('.', '!', '?')):\n",
    "                                    safe_text += \".\"\n",
    "                            \n",
    "                            # FINAL SAFETY: If still problematic, use hardcoded phrase\n",
    "                            if len(safe_text) < 3 or len(safe_text) > 50:\n",
    "                                safe_text = \"Okay.\"\n",
    "                            \n",
    "                            print(f\"ü§ñ AI: {safe_text}\")\n",
    "\n",
    "                            response_queue.put(safe_text)\n",
    "                        except Exception as e:\n",
    "                            print(f\"‚ùå Error: {e}\")\n",
    "                            import traceback\n",
    "                            traceback.print_exc()\n",
    "\n",
    "                    # LAUNCH RESPONSE THREAD\n",
    "                    threading.Thread(target=generate_response, args=(turn_frames,), daemon=True).start()\n",
    "\n",
    "        # ---- BUFFER AUDIO FOR STREAMING AND FINAL TRANSCRIPTION ----\n",
    "        if state in (\"SPEAKING\", \"PAUSING\"):\n",
    "            # For final transcription (never trimmed until turn ends)\n",
    "            with turn_audio_lock:\n",
    "                turn_audio.append((frame.copy(), now))\n",
    "            # For streaming partials\n",
    "            with asr_lock:\n",
    "                asr_audio.append((frame.copy(), now))\n",
    "                if TRANSCRIPTION_MODE == \"whisper\":\n",
    "                    cutoff = now - WHISPER_WINDOW_SEC\n",
    "                    while asr_audio and asr_audio[0][1] < cutoff:\n",
    "                        asr_audio.popleft()\n",
    "            # Vosk internal buffer\n",
    "            if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                if not hasattr(asr_worker, \"vosk_buf\"):\n",
    "                    asr_worker.vosk_buf = np.zeros(0, dtype=np.float32)\n",
    "                asr_worker.vosk_buf = np.concatenate([asr_worker.vosk_buf, frame])\n",
    "                while len(asr_worker.vosk_buf) >= VOSK_MIN_SAMPLES:\n",
    "                    chunk_to_send = asr_worker.vosk_buf[:VOSK_MIN_SAMPLES]\n",
    "                    asr_worker.vosk_buf = asr_worker.vosk_buf[VOSK_MIN_SAMPLES:]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"\\nüõë Test stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499235e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
