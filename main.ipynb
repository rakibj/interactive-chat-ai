{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f817fb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m os.environ[\u001b[33m\"\u001b[39m\u001b[33mNUMEXPR_NUM_THREADS\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33m8\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     23\u001b[39m torch.set_num_threads(\u001b[32m8\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mset_num_interop_threads\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m torch.set_grad_enabled(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mRuntimeError\u001b[39m: Error: cannot set number of interop threads after parallel work has started or set_num_interop_threads called"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import torch\n",
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from faster_whisper import WhisperModel\n",
    "import time\n",
    "from collections import deque\n",
    "import threading\n",
    "import json\n",
    "import io\n",
    "import os\n",
    "\n",
    "os.environ[\"LLAMA_CPP_LOG_LEVEL\"] = \"ERROR\"  # Only show errors\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# Optimize CPU threading for inference\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"OPENBLAS_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"MKL_NUM_THREADS\"] = \"8\"\n",
    "os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"8\"\n",
    "os.environ[\"NUMEXPR_NUM_THREADS\"] = \"8\"\n",
    "\n",
    "torch.set_num_threads(8)\n",
    "torch.set_num_interop_threads(1)\n",
    "torch.set_grad_enabled(False)\n",
    "try:\n",
    "    from pocket_tts import TTSModel\n",
    "    POCKET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POCKET_AVAILABLE = False\n",
    "    print(\"‚ö†Ô∏è pocket-tts not installed. Install with: uv add pocket-tts\")\n",
    "\n",
    "# =============================\n",
    "# CONFIGURATION\n",
    "# =============================\n",
    "TRANSCRIPTION_MODE = \"vosk\"  # Options: \"vosk\" (fast partials) or \"whisper\"\n",
    "PROJECT_ROOT = r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\"\n",
    "TTS_MODE = \"pocket\"  # Options: \"pocket\" (neural) or \"powershell\" (system)\n",
    "POCKET_VOICE = \"alba\"  # Options: alba, marius, javert, jean, fantine, cosette, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25a1a9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Silero VAD...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\PC/.cache\\torch\\hub\\snakers4_silero-vad_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Whisper (for final transcription)...\n",
      "Loading Vosk...\n",
      "ASR mode: vosk\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# LOAD MODELS (ALWAYS LOAD WHISPER FOR FINAL TRANSCRIPTION)\n",
    "# =============================\n",
    "print(\"Loading Silero VAD...\")\n",
    "vad_model, _ = torch.hub.load(repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", force_reload=False)\n",
    "\n",
    "print(\"Loading Whisper (for final transcription)...\")\n",
    "whisper = WhisperModel(\n",
    "    r\"D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\whisper\\distil-small.en\",\n",
    "    device=\"cpu\",\n",
    "    compute_type=\"int8\",\n",
    "    local_files_only=True,  # Force local, no hub download\n",
    "    cpu_threads=8\n",
    ")\n",
    "\n",
    "# Load Vosk only if needed\n",
    "vosk_model = None\n",
    "vosk_rec = None\n",
    "if TRANSCRIPTION_MODE == \"vosk\":\n",
    "    from vosk import Model, KaldiRecognizer\n",
    "    print(\"Loading Vosk...\")\n",
    "    vosk_model = Model(\"models/vosk-model-small-en-us-0.15\")\n",
    "    vosk_rec = KaldiRecognizer(vosk_model, 16000)\n",
    "    vosk_rec.SetWords(True)\n",
    "\n",
    "print(f\"ASR mode: {TRANSCRIPTION_MODE}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6b0f242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Checking model path: D:\\Work\\Projects\\AI\\interactive-chat-ai\\models\\llm\\qwen2.5-3b-instruct-q5_k_m.gguf\n",
      "   Exists? True\n",
      "   Size: 2.27 GB\n"
     ]
    }
   ],
   "source": [
    "# GLOBALS (replace old _llm_model/_llm_tokenizer)\n",
    "_llama_model = None\n",
    "GGUF_MODEL_PATH = os.path.join(PROJECT_ROOT, \"models\", \"llm\" ,\"qwen2.5-3b-instruct-q5_k_m.gguf\")  # Adjust path as needed\n",
    "\n",
    "def get_llm():\n",
    "    global _llama_model\n",
    "    if _llama_model is None:\n",
    "        print(\"‚è≥ Loading Qwen2.5-3B (Q5_K_M GGUF) on CPU...\")\n",
    "        try:\n",
    "            _llama_model = Llama(\n",
    "                model_path=GGUF_MODEL_PATH,\n",
    "                n_ctx=2048,\n",
    "                n_threads=8,\n",
    "                n_threads_batch=8,\n",
    "                n_batch=512,\n",
    "                n_gqa=1,  # ‚ö†Ô∏è CRITICAL FOR QWEN\n",
    "                verbose=False,  # Enable loading logs\n",
    "                use_mmap=True,\n",
    "                use_mlock=False,\n",
    "                rope_freq_base=1000000.0\n",
    "            )\n",
    "            print(\"‚úÖ Qwen2.5-3B loaded successfully\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå FAILED to load GGUF model: {e}\")\n",
    "            print(f\"   Check if file exists: {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "            raise  # Force crash to see error\n",
    "    return _llama_model\n",
    "\n",
    "# Add after defining GGUF_MODEL_PATH\n",
    "print(f\"üîç Checking model path: {GGUF_MODEL_PATH}\")\n",
    "print(f\"   Exists? {os.path.exists(GGUF_MODEL_PATH)}\")\n",
    "print(f\"   Size: {os.path.getsize(GGUF_MODEL_PATH) / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "089cd79c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ASR worker started\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# AUDIO SETUP\n",
    "# =============================\n",
    "SAMPLE_RATE = 16000\n",
    "audio_buffer = []\n",
    "VOSK_MIN_SAMPLES = 3200  # 0.2 sec @ 16kHz\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio_buffer.append(indata.copy())\n",
    "\n",
    "stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=1, callback=audio_callback)\n",
    "\n",
    "\n",
    "# =============================\n",
    "# ASR WORKER (STREAMING PARTIALS)\n",
    "# =============================\n",
    "asr_audio = deque()      # For streaming partials (trimmed)\n",
    "turn_audio = deque()     # For final transcription (full turn)\n",
    "asr_lock = threading.Lock()\n",
    "turn_audio_lock = threading.Lock()\n",
    "current_partial_text = \"\"\n",
    "vosk_reset_requested = False\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    audio = np.clip(audio, -1.0, 1.0)\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def asr_worker():\n",
    "    global current_partial_text, vosk_reset_requested\n",
    "    WHISPER_WINDOW_SEC = 1.2\n",
    "\n",
    "    while True:\n",
    "        time.sleep(0.05 if TRANSCRIPTION_MODE == \"vosk\" else 0.7)\n",
    "\n",
    "        if TRANSCRIPTION_MODE == \"whisper\":\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                now = time.time()\n",
    "                recent = [frame for frame, t in asr_audio if now - t <= WHISPER_WINDOW_SEC]\n",
    "            if not recent:\n",
    "                continue\n",
    "            audio_np = np.concatenate(recent)\n",
    "            segments, _ = whisper.transcribe(\n",
    "                audio_np, language=\"en\", vad_filter=False, beam_size=1, temperature=0.0\n",
    "            )\n",
    "            text = \" \".join(seg.text for seg in segments).strip()\n",
    "            if text and text != current_partial_text:\n",
    "                current_partial_text = text\n",
    "                print(\"üìù Partial:\", text)\n",
    "\n",
    "        else:  # Vosk mode\n",
    "            if vosk_reset_requested:\n",
    "                vosk_rec.Reset()\n",
    "                vosk_reset_requested = False\n",
    "                current_partial_text = \"\"\n",
    "            with asr_lock:\n",
    "                if not asr_audio:\n",
    "                    continue\n",
    "                frame, _ = asr_audio.popleft()\n",
    "                if len(frame) < VOSK_MIN_SAMPLES:\n",
    "                    continue\n",
    "                pcm16 = float32_to_int16(frame)\n",
    "            try:\n",
    "                if vosk_rec.AcceptWaveform(pcm16.tobytes()):\n",
    "                    res = json.loads(vosk_rec.Result())\n",
    "                    text = res.get(\"text\", \"\").strip()\n",
    "                    if text:\n",
    "                        print(\"üìù Final:\", text)\n",
    "                        current_partial_text = \"\"\n",
    "                else:\n",
    "                    res = json.loads(vosk_rec.PartialResult())\n",
    "                    partial = res.get(\"partial\", \"\").strip()\n",
    "                    if partial and partial != current_partial_text:\n",
    "                        current_partial_text = partial\n",
    "                        print(\"üìù Partial:\", partial)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "threading.Thread(target=asr_worker, daemon=True).start()\n",
    "print(\"ASR worker started\")\n",
    "\n",
    "# =============================\n",
    "# TURN-TAKING RULES\n",
    "# =============================\n",
    "TRAILING_CONJUNCTIONS = {\"and\",\"or\",\"but\",\"because\",\"so\",\"that\",\"which\",\"who\",\"when\",\"if\",\"though\",\"while\"}\n",
    "OPEN_ENDED_PREFIXES = (\"i think\",\"i guess\",\"i'm not sure\",\"the thing is\",\"it depends\")\n",
    "QUESTION_LEADINS = (\"do you think\",\"would you say\",\"is it possible\",\"can you\")\n",
    "SELF_REPAIR_MARKERS = (\"i mean\",\"actually\",\"sorry\",\"no wait\")\n",
    "FILLER_ENDINGS = (\"uh\",\"um\",\"like\",\"you know\",\"kind of\")\n",
    "\n",
    "def lexical_bias(text: str) -> float:\n",
    "    if not text: return 0.0\n",
    "    t = text.lower().strip()\n",
    "    words = t.split()\n",
    "    score = 0.0\n",
    "    if words[-1] in TRAILING_CONJUNCTIONS: score -= 1.0\n",
    "    if any(t.startswith(p) for p in OPEN_ENDED_PREFIXES): score -= 0.6\n",
    "    if any(t.startswith(q) for q in QUESTION_LEADINS): score -= 0.5\n",
    "    if any(m in t[-20:] for m in SELF_REPAIR_MARKERS): score -= 0.4\n",
    "    if words[-1] in FILLER_ENDINGS: score -= 0.7\n",
    "    return score\n",
    "\n",
    "def energy_decay_score(energy_history):\n",
    "    if len(energy_history) < 5: return 0.0\n",
    "    x = np.arange(len(energy_history))\n",
    "    y = np.array(energy_history)\n",
    "    slope = np.polyfit(x, y, 1)[0]\n",
    "    return 0.8 if slope < -0.00015 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a017d0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# POCKET TTS LOADING\n",
    "# =============================\n",
    "_pocket_model = None\n",
    "_pocket_voice_state = None\n",
    "_pocket_sample_rate = 24000  # Pocket TTS default\n",
    "\n",
    "def get_pocket_tts():\n",
    "    \"\"\"Lazy load Pocket TTS model\"\"\"\n",
    "    global _pocket_model, _pocket_voice_state, _pocket_sample_rate\n",
    "    if _pocket_model is None:\n",
    "        if not POCKET_AVAILABLE:\n",
    "            raise ImportError(\"pocket-tts not installed\")\n",
    "        print(f\"‚è≥ Loading Pocket TTS (voice: {POCKET_VOICE})...\")\n",
    "        _pocket_model = TTSModel.load_model()\n",
    "        _pocket_voice_state = _pocket_model.get_state_for_audio_prompt(POCKET_VOICE)\n",
    "        _pocket_sample_rate = _pocket_model.sample_rate\n",
    "        print(\"‚úÖ Pocket TTS loaded!\")\n",
    "    return _pocket_model, _pocket_voice_state, _pocket_sample_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5f5c3275",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "def speak(text):\n",
    "    \"\"\"Speak text using configured TTS backend\"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return\n",
    "    \n",
    "    text = text.strip()\n",
    "    if TTS_MODE == \"pocket\":\n",
    "        try:\n",
    "            model, voice_state, sr = get_pocket_tts()\n",
    "            audio = model.generate_audio(voice_state, text)\n",
    "            audio_np = audio.numpy() if hasattr(audio, 'numpy') else np.array(audio)\n",
    "            if audio_np.max() > 1.0:\n",
    "                audio_np = audio_np / 32767.0\n",
    "            sd.play(audio_np, sr)\n",
    "            sd.wait()\n",
    "        except Exception as e:\n",
    "            print(f\"üîä Pocket TTS error: {e}\")\n",
    "            speak_powershell(text)\n",
    "    else:\n",
    "        speak_powershell(text)\n",
    "\n",
    "def speak_powershell(text):\n",
    "    \"\"\"Original Windows PowerShell TTS (fallback)\"\"\"\n",
    "    safe_text = text.replace('\"', '\"\"').replace('\\n', ' ').replace('\\r', '')\n",
    "    cmd = f'Add-Type -AssemblyName System.Speech; $s=New-Object System.Speech.Synthesis.SpeechSynthesizer; $s.Speak(\"{safe_text}\")'\n",
    "    try:\n",
    "        subprocess.run([\"powershell\", \"-Command\", cmd],\n",
    "                       stdout=subprocess.DEVNULL,\n",
    "                       stderr=subprocess.DEVNULL,\n",
    "                       timeout=10)\n",
    "    except Exception as e:\n",
    "        print(f\"üîä Speech error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4d8cb50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================\n",
    "# WINDOWS-RELIABLE TTS (POWER SHELL)\n",
    "# =============================\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "response_queue = queue.Queue()\n",
    "\n",
    "def tts_main_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            text = response_queue.get(timeout=0.1)\n",
    "            print(f\"üó£Ô∏è Speaking: '{text}'\")\n",
    "            speak(text)  # This blocks until audio finishes\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "\n",
    "# Started earlier in your code:\n",
    "threading.Thread(target=tts_main_loop, daemon=False).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6e1d046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At the TOP of your notebook (before main loop), make sure these exist:\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class TurnTiming:\n",
    "    turn_id: int = 0\n",
    "    speech_end_time: float = 0.0\n",
    "    audio_capture_duration_ms: float = 0.0\n",
    "    whisper_transcribe_ms: float = 0.0\n",
    "    whisper_rtf: float = 0.0\n",
    "    llm_tokenize_ms: float = 0.0\n",
    "    llm_generate_ms: float = 0.0\n",
    "    llm_tokens_per_sec: float = 0.0\n",
    "    text_process_ms: float = 0.0\n",
    "    tts_generate_ms: float = 0.0\n",
    "    tts_playback_ms: float = 0.0\n",
    "    total_latency_ms: float = 0.0\n",
    "    total_audio_duration_sec: float = 0.0\n",
    "    \n",
    "    def print_report(self):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"üìä TURN #{self.turn_id} TIMING AUDIT\")\n",
    "        print(f\"{'='*60}\")\n",
    "        print(f\"üéôÔ∏è  User audio duration:     {self.total_audio_duration_sec:.2f}s\")\n",
    "        print(f\"‚è±Ô∏è  Speech end ‚Üí Response:   {self.total_latency_ms:.0f}ms total\")\n",
    "        print(f\"{'‚îÄ'*40}\")\n",
    "        print(f\"1. Audio buffer capture:     {self.audio_capture_duration_ms:.1f}ms\")\n",
    "        print(f\"2. Whisper transcription:    {self.whisper_transcribe_ms:.1f}ms (RTF: {self.whisper_rtf:.2f}x)\")\n",
    "        print(f\"3. LLM tokenization:         {self.llm_tokenize_ms:.1f}ms\")\n",
    "        print(f\"4. LLM generation:           {self.llm_generate_ms:.1f}ms ({self.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        print(f\"5. Text processing:          {self.text_process_ms:.1f}ms\")\n",
    "        if self.tts_generate_ms > 0:\n",
    "            print(f\"6. TTS generation:           {self.tts_generate_ms:.1f}ms\")\n",
    "            print(f\"7. Audio playback:           {self.tts_playback_ms:.1f}ms\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "\n",
    "turn_counter = 0\n",
    "timing_history: List[TurnTiming] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8931e51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(frames, timing: TurnTiming):\n",
    "    global turn_counter\n",
    "    timing.speech_end_time = time.perf_counter()\n",
    "    \n",
    "    try:\n",
    "        # Stage 1: Audio Capture\n",
    "        t0 = time.perf_counter()\n",
    "        if not frames:\n",
    "            print(\"‚ö†Ô∏è No audio captured ‚Äî skipping response\")\n",
    "            return\n",
    "        \n",
    "        full_audio = np.concatenate([frame for frame, _ in frames])\n",
    "        timing.total_audio_duration_sec = full_audio.shape[0] / 16000.0\n",
    "        timing.audio_capture_duration_ms = (time.perf_counter() - t0) * 1000\n",
    "        print(f\"üîä Captured {len(frames)} frames ({timing.total_audio_duration_sec:.2f}s) in {timing.audio_capture_duration_ms:.1f}ms\")\n",
    "        \n",
    "        # Stage 2: Whisper Transcription\n",
    "        t1 = time.perf_counter()\n",
    "        segments, info = whisper.transcribe(\n",
    "            full_audio,\n",
    "            language=\"en\",\n",
    "            beam_size=5,\n",
    "            temperature=0.0,\n",
    "            condition_on_previous_text=False\n",
    "        )\n",
    "        user_text = \" \".join(seg.text for seg in segments).strip()\n",
    "        timing.whisper_transcribe_ms = (time.perf_counter() - t1) * 1000\n",
    "        timing.whisper_rtf = timing.whisper_transcribe_ms / (timing.total_audio_duration_sec * 1000)\n",
    "        \n",
    "        if not user_text:\n",
    "            print(\"‚ö†Ô∏è Empty transcription ‚Äî skipping response\")\n",
    "            return\n",
    "        print(f\"üí¨ User: '{user_text}' (Whisper: {timing.whisper_transcribe_ms:.1f}ms, RTF: {timing.whisper_rtf:.2f}x)\")\n",
    "        \n",
    "        # Stage 3: LLM Generation (STREAMING)\n",
    "        llm_model = get_llm()\n",
    "        t3 = time.perf_counter()\n",
    "        \n",
    "        SYSTEM_PROMPT = (\n",
    "            \"You are a helpful, concise AI assistant for real-time voice conversations. \"\n",
    "            \"Keep responses under 2 sentences. Speak naturally like a human. \"\n",
    "            \"Never say 'As an AI...' or mention your limitations.\"\n",
    "        )\n",
    "        \n",
    "        stream = llm_model.create_chat_completion(\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "                {\"role\": \"user\", \"content\": user_text}\n",
    "            ],\n",
    "            max_tokens=40,\n",
    "            temperature=0.0,\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        full_response_text = \"\"  # Track complete response for timing\n",
    "        sentence_buffer = \"\"\n",
    "        \n",
    "        for chunk in stream:\n",
    "            if \"choices\" in chunk and len(chunk[\"choices\"]) > 0:\n",
    "                delta = chunk[\"choices\"][0].get(\"delta\", {})\n",
    "                if \"content\" in delta:\n",
    "                    token = delta[\"content\"]\n",
    "                    full_response_text += token\n",
    "                    sentence_buffer += token\n",
    "                    \n",
    "                    # Speak complete sentences immediately\n",
    "                    if token in \".!?\":\n",
    "                        sentence = sentence_buffer.strip()\n",
    "                        if sentence:\n",
    "                            response_queue.put(sentence)\n",
    "                        sentence_buffer = \"\"\n",
    "        \n",
    "        # Handle any remaining text\n",
    "        if sentence_buffer.strip():\n",
    "            response_queue.put(sentence_buffer.strip())\n",
    "            full_response_text += sentence_buffer\n",
    "        \n",
    "        # Calculate timing metrics\n",
    "        gen_time = time.perf_counter() - t3\n",
    "        timing.llm_generate_ms = gen_time * 1000\n",
    "        output_tokens = len(full_response_text.split())\n",
    "        timing.llm_tokens_per_sec = output_tokens / gen_time if gen_time > 0 else 0\n",
    "        print(f\"ü§ñ LLM: {timing.llm_generate_ms:.1f}ms ({timing.llm_tokens_per_sec:.1f} tok/s)\")\n",
    "        \n",
    "        # Final timing report (no TTS timing since it's handled by queue)\n",
    "        timing.total_latency_ms = (time.perf_counter() - timing.speech_end_time) * 1000\n",
    "        timing.print_report()\n",
    "        timing_history.append(timing)\n",
    "        turn_counter += 1\n",
    "        \n",
    "        # Running statistics\n",
    "        if len(timing_history) > 1:\n",
    "            avg_latency = sum(t.total_latency_ms for t in timing_history) / len(timing_history)\n",
    "            print(f\"üìà Running average latency: {avg_latency:.0f}ms over {len(timing_history)} turns\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        response_queue.put(\"Sorry, I couldn't process that.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20b88039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéôÔ∏è Real-time conversation test started\n",
      "üü¢ Speech started\n",
      "üü° Pause 652 ms\n",
      "üî¥ Turn ended (confidence=1.70, silence=1225ms)\n",
      "üîä Captured 71 frames (2.27s) in 0.1ms\n",
      "üí¨ User: 'Hey, how's it going?' (Whisper: 878.5ms, RTF: 0.39x)\n",
      "üó£Ô∏è Speaking: 'Hey there!'\n",
      "ü§ñ LLM: 648.9ms (12.3 tok/s)\n",
      "\n",
      "============================================================\n",
      "üìä TURN #4 TIMING AUDIT\n",
      "============================================================\n",
      "üéôÔ∏è  User audio duration:     2.27s\n",
      "‚è±Ô∏è  Speech end ‚Üí Response:   1529ms total\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "1. Audio buffer capture:     0.1ms\n",
      "2. Whisper transcription:    878.5ms (RTF: 0.39x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           648.9ms (12.3 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "üìà Running average latency: 1287ms over 4 turns\n",
      "üó£Ô∏è Speaking: 'Going well, thanks.'\n",
      "üó£Ô∏è Speaking: 'How about you?'\n",
      "üü¢ Speech started\n",
      "üü° Pause 603 ms\n",
      "üî¥ Turn ended (confidence=1.70, silence=1226ms)\n",
      "üîä Captured 129 frames (4.13s) in 0.2ms\n",
      "üí¨ User: 'Do you well? Tell me more about yourself. What can I do?' (Whisper: 921.7ms, RTF: 0.22x)\n",
      "üó£Ô∏è Speaking: 'I'm functioning well, thanks!'\n",
      "ü§ñ LLM: 1612.5ms (12.4 tok/s)\n",
      "\n",
      "============================================================\n",
      "üìä TURN #6 TIMING AUDIT\n",
      "============================================================\n",
      "üéôÔ∏è  User audio duration:     4.13s\n",
      "‚è±Ô∏è  Speech end ‚Üí Response:   2536ms total\n",
      "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
      "1. Audio buffer capture:     0.2ms\n",
      "2. Whisper transcription:    921.7ms (RTF: 0.22x)\n",
      "3. LLM tokenization:         0.0ms\n",
      "4. LLM generation:           1612.5ms (12.4 tok/s)\n",
      "5. Text processing:          0.0ms\n",
      "============================================================\n",
      "\n",
      "üìà Running average latency: 1536ms over 5 turns\n",
      "üó£Ô∏è Speaking: 'I can answer questions, provide information, and assist with tasks.'\n",
      "üó£Ô∏è Speaking: 'How can I help you today?'\n",
      "\n",
      "üõë Test stopped\n"
     ]
    }
   ],
   "source": [
    "# =============================\n",
    "# MAIN LOOP\n",
    "# =============================\n",
    "import tempfile\n",
    "import os\n",
    "import wave\n",
    "import time\n",
    "import re\n",
    "\n",
    "# CONFIG\n",
    "VAD_MIN_SAMPLES = 512\n",
    "PAUSE_MS = 600\n",
    "END_MS = 1200\n",
    "SAFETY_TIMEOUT_MS = 2500\n",
    "ENERGY_FLOOR = 0.015\n",
    "WHISPER_WINDOW_SEC = 3.0\n",
    "CONFIDENCE_THRESHOLD = 1.2\n",
    "\n",
    "# STATE\n",
    "state = \"IDLE\"\n",
    "last_voice_time = None\n",
    "last_ai_interrupted = False\n",
    "vad_buffer = np.zeros(0, dtype=np.float32)\n",
    "energy_history = deque(maxlen=15)\n",
    "pause_history = deque(maxlen=5)\n",
    "micro_spike_times = deque(maxlen=5)\n",
    "\n",
    "stream.start()\n",
    "print(\"üéôÔ∏è Real-time conversation test started\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        if not audio_buffer:\n",
    "            time.sleep(0.01)\n",
    "            continue\n",
    "\n",
    "        # ---- COLLECT AUDIO CHUNK ----\n",
    "        chunk = audio_buffer.pop(0).astype(np.float32).flatten()\n",
    "        vad_buffer = np.concatenate([vad_buffer, chunk])\n",
    "\n",
    "        if len(vad_buffer) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        frame = vad_buffer[:VAD_MIN_SAMPLES]\n",
    "        vad_buffer = vad_buffer[VAD_MIN_SAMPLES:]\n",
    "        if len(frame) < VAD_MIN_SAMPLES:\n",
    "            continue\n",
    "\n",
    "        now = time.time()\n",
    "        rms = np.sqrt(np.mean(frame ** 2))\n",
    "        energy_history.append(rms)\n",
    "\n",
    "        # ---- VAD ----\n",
    "        with torch.no_grad():\n",
    "            vad_confidence = vad_model(torch.from_numpy(frame).unsqueeze(0), 16000).item()\n",
    "        speech_started = vad_confidence > 0.5\n",
    "        sustained = sum(e > ENERGY_FLOOR for e in energy_history) >= 3\n",
    "\n",
    "        # ---- MICRO-SPIKE DETECTION ----\n",
    "        if state == \"PAUSING\" and rms > ENERGY_FLOOR:\n",
    "            micro_spike_times.append(now)\n",
    "\n",
    "        # ---- STATE MACHINE ----\n",
    "        if state == \"IDLE\":\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"üü¢ Speech started\")\n",
    "\n",
    "        elif state == \"SPEAKING\":\n",
    "            if speech_started or sustained:\n",
    "                last_voice_time = now\n",
    "            else:\n",
    "                elapsed = (now - last_voice_time) * 1000\n",
    "                if elapsed >= PAUSE_MS:\n",
    "                    state = \"PAUSING\"\n",
    "                    print(f\"üü° Pause {int(elapsed)} ms\")\n",
    "\n",
    "        elif state == \"PAUSING\":\n",
    "            elapsed = (now - last_voice_time) * 1000\n",
    "\n",
    "            # SAFETY TIMEOUT\n",
    "            if elapsed > SAFETY_TIMEOUT_MS:\n",
    "                print(f\"üî¥ SAFETY TIMEOUT: Force-ending turn after {elapsed:.0f}ms\")\n",
    "                state = \"IDLE\"\n",
    "                last_voice_time = None\n",
    "                energy_history.clear()\n",
    "                pause_history.clear()\n",
    "                micro_spike_times.clear()\n",
    "                last_ai_interrupted = False\n",
    "                with turn_audio_lock:\n",
    "                    turn_audio.clear()\n",
    "                current_partial_text = \"\"\n",
    "                if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                    vosk_reset_requested = True\n",
    "                continue\n",
    "\n",
    "            # RESUME SPEECH?\n",
    "            if speech_started or sustained:\n",
    "                state = \"SPEAKING\"\n",
    "                last_voice_time = now\n",
    "                print(\"üü¢ Speech resumed\")\n",
    "            else:\n",
    "                # CALCULATE CONFIDENCE\n",
    "                confidence = 0.0\n",
    "                if elapsed > END_MS:\n",
    "                    confidence += 1.0\n",
    "                if len(energy_history) >= 8:\n",
    "                    recent_energies = list(energy_history)[-8:]\n",
    "                    if max(recent_energies) < ENERGY_FLOOR * 1.8:\n",
    "                        confidence += 0.7\n",
    "                if elapsed < 1000:\n",
    "                    recent_spikes = [t for t in micro_spike_times if now - t < 0.6]\n",
    "                    if len(recent_spikes) >= 2:\n",
    "                        confidence -= 0.5\n",
    "                if elapsed < 900 and current_partial_text:\n",
    "                    confidence += lexical_bias(current_partial_text) * 0.6\n",
    "                if last_ai_interrupted:\n",
    "                    confidence -= 0.5\n",
    "\n",
    "                # END TURN?\n",
    "                if confidence >= CONFIDENCE_THRESHOLD:\n",
    "                    print(f\"üî¥ Turn ended (confidence={confidence:.2f}, silence={elapsed:.0f}ms)\")\n",
    "\n",
    "                    # CAPTURE FULL TURN AUDIO\n",
    "                    with turn_audio_lock:\n",
    "                        turn_frames = list(turn_audio)\n",
    "                        turn_audio.clear()\n",
    "\n",
    "                    # RESET STATE\n",
    "                    state = \"IDLE\"\n",
    "                    last_voice_time = None\n",
    "                    energy_history.clear()\n",
    "                    pause_history.clear()\n",
    "                    micro_spike_times.clear()\n",
    "                    last_ai_interrupted = False\n",
    "                    current_partial_text = \"\"\n",
    "                    if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                        vosk_reset_requested = True\n",
    "\n",
    "                    # FIX: Create timing object and pass it to the thread\n",
    "                    timing = TurnTiming(turn_id=turn_counter)\n",
    "                    threading.Thread(\n",
    "                        target=generate_response, \n",
    "                        args=(turn_frames, timing),  # <-- Pass both arguments!\n",
    "                        daemon=True\n",
    "                    ).start()\n",
    "                    \n",
    "                    turn_counter += 1  # Increment for next turn\n",
    "\n",
    "                   \n",
    "\n",
    "\n",
    "        # ---- BUFFER AUDIO FOR STREAMING AND FINAL TRANSCRIPTION ----\n",
    "        if state in (\"SPEAKING\", \"PAUSING\"):\n",
    "            # For final transcription (never trimmed until turn ends)\n",
    "            with turn_audio_lock:\n",
    "                turn_audio.append((frame.copy(), now))\n",
    "            # For streaming partials\n",
    "            with asr_lock:\n",
    "                asr_audio.append((frame.copy(), now))\n",
    "                if TRANSCRIPTION_MODE == \"whisper\":\n",
    "                    cutoff = now - WHISPER_WINDOW_SEC\n",
    "                    while asr_audio and asr_audio[0][1] < cutoff:\n",
    "                        asr_audio.popleft()\n",
    "            # Vosk internal buffer\n",
    "            if TRANSCRIPTION_MODE == \"vosk\":\n",
    "                if not hasattr(asr_worker, \"vosk_buf\"):\n",
    "                    asr_worker.vosk_buf = np.zeros(0, dtype=np.float32)\n",
    "                asr_worker.vosk_buf = np.concatenate([asr_worker.vosk_buf, frame])\n",
    "                while len(asr_worker.vosk_buf) >= VOSK_MIN_SAMPLES:\n",
    "                    chunk_to_send = asr_worker.vosk_buf[:VOSK_MIN_SAMPLES]\n",
    "                    asr_worker.vosk_buf = asr_worker.vosk_buf[VOSK_MIN_SAMPLES:]\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    stream.stop()\n",
    "    print(\"\\nüõë Test stopped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5499235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% =============================\n",
    "# BENCHMARK SUMMARY TOOL\n",
    "# =============================\n",
    "def print_benchmark_summary():\n",
    "    \"\"\"Call this manually after a session to see aggregate stats\"\"\"\n",
    "    if not timing_history:\n",
    "        print(\"No timing data recorded yet\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"üìä SESSION BENCHMARK SUMMARY ({len(timing_history)} turns)\")\n",
    "    print(f\"{'='*70}\")\n",
    "    \n",
    "    stages = [\n",
    "        (\"Audio Capture\", \"audio_capture_duration_ms\"),\n",
    "        (\"Whisper Transcribe\", \"whisper_transcribe_ms\"),\n",
    "        (\"LLM Tokenization\", \"llm_tokenize_ms\"),\n",
    "        (\"LLM Generation\", \"llm_generate_ms\"),\n",
    "        (\"Text Processing\", \"text_process_ms\"),\n",
    "        (\"TTS Generation\", \"tts_generate_ms\"),\n",
    "        (\"Total Latency\", \"total_latency_ms\")\n",
    "    ]\n",
    "    \n",
    "    for name, attr in stages:\n",
    "        values = [getattr(t, attr) for t in timing_history if getattr(t, attr) > 0]\n",
    "        if values:\n",
    "            avg = sum(values) / len(values)\n",
    "            mn, mx = min(values), max(values)\n",
    "            print(f\"{name:20s}: {avg:6.1f}ms avg [{mn:6.1f} - {mx:6.1f}]\")\n",
    "    \n",
    "    # RTF analysis\n",
    "    rtfs = [t.whisper_rtf for t in timing_history if t.whisper_rtf > 0]\n",
    "    if rtfs:\n",
    "        print(f\"\\nWhisper RTF: {sum(rtfs)/len(rtfs):.2f}x (lower is better, <1.0 = real-time)\")\n",
    "    \n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Run this anytime to see stats:\n",
    "print_benchmark_summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
