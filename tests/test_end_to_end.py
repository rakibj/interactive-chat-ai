#!/usr/bin/env python3
"""End-to-end test: LLM response signals with actual LLM backend"""
import sys
sys.path.insert(0, "interactive_chat")

from config import get_system_prompt, INSTRUCTION_PROFILES
from interfaces.llm import get_llm, extract_signals_from_response
from core.signals import get_signal_registry
from signals.consumer import handle_signal
import json

print("=" * 70)
print("END-TO-END TEST: LLM Response Signal Emission")
print("=" * 70)

# Step 1: Verify system prompt includes signal guidance
print("\nüìã STEP 1: Verify system prompt includes signals guidance")
prompt = get_system_prompt("negotiator")
if "OPTIONAL STRUCTURED SIGNALS" in prompt:
    print("‚úÖ Base prompt has signal guidance")
if "SIGNALS YOU MAY EMIT:" in prompt:
    print("‚úÖ Profile signals injected in system prompt")
    # Show the signals section
    idx = prompt.find("SIGNALS YOU MAY EMIT:")
    print("\n---\n" + prompt[idx:idx+300] + "\n---\n")

# Step 2: Set up signal consumer
print("üìã STEP 2: Register signal consumer")
registry = get_signal_registry()
registry.register_all(handle_signal)
print("‚úÖ Consumer registered (will log all signals)")

# Step 3: Prepare LLM call
print("\nüìã STEP 3: Call LLM with signal-capable prompt")
print("Profile: negotiator")
print("LLM Backend: " + get_llm().__class__.__name__)

# Build messages
messages = [
    {
        "role": "system",
        "content": get_system_prompt("negotiator")
    },
    {
        "role": "user",
        "content": "What's the lowest price you'll accept?"
    }
]

print("\nüîÑ Generating LLM response...")
print("-" * 70)

# Stream completion with signal emission
llm = get_llm()
response_text = ""

try:
    for token in llm.stream_completion(
        messages=messages,
        max_tokens=50,
        temperature=0.5,
        emit_signals=True  # Enable signal emission
    ):
        response_text += token
        print(token, end="", flush=True)
    
    print("\n" + "-" * 70)
    print(f"\n‚úÖ LLM response received ({len(response_text)} chars)")
    
    # Step 4: Check for signals in response
    print("\nüìã STEP 4: Extract signals from response")
    signals = extract_signals_from_response(response_text)
    
    if signals:
        print(f"‚úÖ Found {len(signals)} signal(s):")
        for signal_name, payload in signals.items():
            print(f"\n   Signal: {signal_name}")
            print(f"   Payload: {json.dumps(payload, indent=6)}")
    else:
        print("‚ÑπÔ∏è  No signals in response (this is valid - signals are optional)")
    
    # Step 5: Show what signal consumer would have logged
    print("\nüìã STEP 5: Signal consumer output")
    print("(Consumer logs all signals to stdout automatically)")
    if signals:
        print("‚úÖ Signals would be logged above in [SIGNAL] blocks")
    else:
        print("‚ÑπÔ∏è  No signals to log")
    
    print("\n" + "=" * 70)
    print("‚úÖ END-TO-END TEST COMPLETE")
    print("=" * 70)
    print("\nWhat happened:")
    print("1. ‚úÖ System prompt trained LLM on signal format")
    print("2. ‚úÖ Profile signals injected in system prompt")
    print("3. ‚úÖ LLM generated response (with optional signals)")
    print("4. ‚úÖ stream_completion() emitted LLM lifecycle signals")
    print("5. ‚úÖ Signal consumer registered (would log to stdout)")
    print("6. ‚úÖ Signals extracted from response")
    print("7. ‚úÖ Extracted signals emitted via registry")
    
except Exception as e:
    print(f"\n‚ùå Error: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
